{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8a57c4-7789-49b2-a821-1c6e91ab09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from itertools import product\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be158c4-2650-4165-b73c-d9b64c896f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 12) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "# Impostazioni per la stampa dei numeri\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "# Caricamento dei dati\n",
    "my_data = np.genfromtxt('ML-CUP24-TR.csv', delimiter=',')\n",
    "X = my_data[:, 1:13]\n",
    "y = my_data[:, 13:16]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# Suddividi i dati in train (60%) e temp (40%) (HOLDOUT)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Suddividi temp in validation (20%) e test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalizza i dati\n",
    "scaler_X = StandardScaler().fit(X_train)\n",
    "scaler_y = StandardScaler().fit(y_train)\n",
    "\n",
    "X_train = scaler_X.transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d713ceb-604a-49d0-b3ab-afd2dfad3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di attivazione RBF con TensorFlow\n",
    "def rbf_activation(x, centers, gamma=1.0):\n",
    "    # Calcola la distanza euclidea tra l'input e i centri\n",
    "    diff = x[:, np.newaxis, :] - centers\n",
    "    dist_sq = tf.reduce_sum(tf.square(diff), axis=-1)  # Usa tf.reduce_sum invece di np.sum\n",
    "    return tf.exp(-gamma * dist_sq)\n",
    "\n",
    "# Creare il modello\n",
    "def build_rbf_model(input_dim, output_dim, n_centers=10, gamma=1.0):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Layer di input: utilizza 'shape' invece di 'input_dim'\n",
    "    model.add(layers.InputLayer(shape=(input_dim,)))\n",
    "\n",
    "    # Layer nascosta con attivazione RBF (calcola distanza dal centro)\n",
    "    centers = tf.Variable(np.random.randn(n_centers, input_dim), dtype=tf.float32)  # Centri randomici\n",
    "    model.add(layers.Lambda(lambda x: rbf_activation(x, centers, gamma)))\n",
    "\n",
    "    # Layer di output con una dimensione pari al numero di target (3 in questo caso: x, y, z)\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070242e8-f5cf-485e-a1a5-e083bf8551d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9570f04a-fd28-4c38-a9b8-ac599f50e8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9017\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9017\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8891\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8891\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8461\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8461\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.6904\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.6904\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.6500\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.6500\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8152\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8152\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8947\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8947\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 1.0383\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 1.0383\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.9307\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9307\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8111\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8111\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9397\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9397\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9165\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9165\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8944\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8944\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8967\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8967\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8977\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8977\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8732\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8732\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8651\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8651\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8889\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8889\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8971\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8971\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8967\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8967\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8986\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8986\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8968\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8968\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8915\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8915\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8979\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8979\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8969\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8969\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8946\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8946\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8963\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8963\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8960\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8960\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8957\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8957\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8978\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8978\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8960\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8960\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8961\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8961\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8964\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8964\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8954\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8954\n",
      "Testing parameters: {'n_centers': 5, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8954\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8954\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.7857\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.7857\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.7927\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.7927\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8197\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8197\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.4172\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.4172\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.5091\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.5091\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.5104\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.5104\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9892\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9892\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 1.0754\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.0754\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.0309\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.0309\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8543\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.8543\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8793\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8793\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8932\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8932\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8888\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8888\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8972\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Test MSE: 0.8972\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8945\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8945\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8326\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8326\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8883\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8883\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8402\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8402\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8972\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8972\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8959\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8959\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8965\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8965\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9026\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.9026\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8967\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8967\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8986\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8986\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8959\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8959\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8971\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.8971\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8980\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8980\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8955\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8955\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8980\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8980\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8942\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8942\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8964\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8964\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8966\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8966\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8957\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8957\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8767\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8767\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8713\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8713\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.9156\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9156\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.3263\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.3263\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.3814\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Test MSE: 0.3814\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.4402\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.4402\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9092\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9092\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8894\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8894\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8902\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8902\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8053\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8053\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9909\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9909\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.7822\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.7822\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8933\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8933\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8947\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.8947\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8951\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8951\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8662\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8662\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8584\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8584\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8841\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8841\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8955\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8955\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8944\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8944\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8978\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8978\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8937\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8937\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8954\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8954\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.5, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8972\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8972\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8934\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8934\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8968\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8968\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8957\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8957\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8938\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8938\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8943\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8943\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8918\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8918\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8972\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8972\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8962\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8962\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8966\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8966\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8975\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8975\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 1.0, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8960\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8960\n",
      "Grid Search completata. Risultati salvati in grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Funzione per eseguire la Grid Search\n",
    "def grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid, epochs=50):\n",
    "    results = []\n",
    "    \n",
    "    for params in product(*param_grid.values()):\n",
    "        # Costruire il modello con i parametri correnti\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Testing parameters: {param_dict}\")\n",
    "        \n",
    "        model = build_rbf_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=y_train.shape[1],\n",
    "            n_centers=param_dict[\"n_centers\"],\n",
    "            gamma=param_dict[\"gamma\"]\n",
    "        )\n",
    "        \n",
    "        # Configurare l'ottimizzatore\n",
    "        optimizer = param_dict[\"optimizer\"](learning_rate=param_dict[\"learning_rate\"])\n",
    "        \n",
    "        # Passare il valore di batch_size\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        \n",
    "        # Compilare e allenare il modello\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Ottenere i valori di loss per train e validation\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Valutare il modello sui dati di test\n",
    "        test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Calcolare l'MSE sui dati di test\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        \n",
    "        # Salvare i risultati\n",
    "        results.append({\n",
    "            **param_dict,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_mse\": test_mse\n",
    "        })\n",
    "    \n",
    "    # Salvare i risultati in un file CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"TestsData/grid_search_results.csv\", index=False)\n",
    "    print(\"Grid Search completata. Risultati salvati in grid_search_results.csv\")\n",
    "\n",
    "# Definizione della griglia dei parametri\n",
    "param_grid = {\n",
    "    \"n_centers\": [5, 10, 15],  # Numero di centri\n",
    "    \"gamma\": [0.1, 0.5, 1.0],  # Gamma per RBF\n",
    "    \"optimizer\": [optimizers.Adam, optimizers.SGD],  # Ottimizzatori\n",
    "    \"learning_rate\": [0.001, 0.01],  # Learning rate\n",
    "    \"batch_size\": [32, 64, 128]  # Diverse batch size\n",
    "}\n",
    "\n",
    "# Eseguire la Grid Search\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad59d20-7d35-4ab2-9000-886cd24b47d4",
   "metadata": {},
   "source": [
    "### Analisi result1:\n",
    "Migliori valori con cui si raggiungono i migliori risultati:\n",
    "- Numero di centri alto (10-15)\n",
    "- Gamma basso (0.1)\n",
    "- Learning rate alto (0.01)\n",
    "- Batch size (indifferente - tendente basso 32)\n",
    "- Adam è il migliore in assoluto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e9a43-7443-452c-a634-5adaec0fb3c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1280cd1-72ee-4870-9fba-ef123d04a572",
   "metadata": {},
   "source": [
    "- N centri: aggiungo 20 tolgo 5\n",
    "- Gamma: tolgo 0.5 e 1 e aggiungo 0.001 e 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "447e94b0-0705-455a-98fa-0e987c7f7766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 1.0619\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 1.0619\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8700\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8700\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8893\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8893\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9028\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9028\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8989\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8989\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8881\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8881\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8814\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.8814\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9565\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9565\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.0064\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.0064\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8941\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8941\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9002\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9002\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8875\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8875\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9565\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.9565\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9048\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9048\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.5537\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 1.5537\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.7938\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.7938\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8017\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8017\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8635\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8635\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8203\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8203\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9165\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Test MSE: 0.9165\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.9046\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9046\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8620\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8620\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9034\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.9034\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9406\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9406\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.7652\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.7652\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9109\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8335\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8335\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.5699\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.5699\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.5032\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.5032\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.5598\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.5598\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8924\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8924\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9282\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.9282\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8979\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8979\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8974\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8974\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8771\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8771\n",
      "Testing parameters: {'n_centers': 10, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9395\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9395\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8978\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8978\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8998\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8998\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.2037\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.2037\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8997\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8997\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9131\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9131\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8805\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8805\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8749\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8749\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9919\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9919\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.1422\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Test MSE: 1.1422\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8947\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8947\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8974\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8974\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9004\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9004\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9450\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.9450\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8915\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8915\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.1046\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 1.1046\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.7950\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.7950\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.7545\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.7545\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8563\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8563\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9923\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9923\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 1.4269\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.4269\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.1290\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.1290\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9211\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9211\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8590\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8590\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8811\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8811\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.7489\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.7489\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8645\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8645\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8743\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8743\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.2693\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.2693\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.3376\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.3376\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.4155\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.4155\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8713\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8713\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9402\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Test MSE: 0.9402\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8837\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8837\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8893\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8893\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8246\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8246\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9126\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9126\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9109\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9627\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9627\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8914\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8914\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9087\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9087\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8965\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8965\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8752\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8752\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8925\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8925\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9267\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9267\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 1.0746\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 1.0746\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9005\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9005\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.8951\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.8951\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.001, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9036\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9036\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.8589\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8589\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9400\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9400\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8752\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8752\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.6696\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.6696\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.7116\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.7116\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8233\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.8233\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.9645\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.9645\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 1.0171\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 1.0171\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.9041\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9041\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.8833\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8833\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9205\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9205\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.01, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.9257\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9257\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 0.6760\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.6760\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.8173\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8173\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.8240\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.8240\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.2641\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.2641\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.3932\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.3932\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.3894\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.3894\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 32}\n",
      "Test Loss: 1.1147\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 1.1147\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 64}\n",
      "Test Loss: 0.9109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9109\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Test Loss: 0.7864\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.7864\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.9291\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.9291\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.9255\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Test MSE: 0.9255\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.sgd.SGD'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.8687\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.8687\n",
      "Grid Search completata. Risultati salvati in grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Funzione per eseguire la Grid Search\n",
    "def grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid, epochs=50):\n",
    "    results = []\n",
    "    \n",
    "    for params in product(*param_grid.values()):\n",
    "        # Costruire il modello con i parametri correnti\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Testing parameters: {param_dict}\")\n",
    "        \n",
    "        model = build_rbf_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=y_train.shape[1],\n",
    "            n_centers=param_dict[\"n_centers\"],\n",
    "            gamma=param_dict[\"gamma\"]\n",
    "        )\n",
    "        \n",
    "        # Configurare l'ottimizzatore\n",
    "        optimizer = param_dict[\"optimizer\"](learning_rate=param_dict[\"learning_rate\"])\n",
    "        \n",
    "        # Passare il valore di batch_size\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        \n",
    "        # Compilare e allenare il modello\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Ottenere i valori di loss per train e validation\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Valutare il modello sui dati di test\n",
    "        test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Calcolare l'MSE sui dati di test\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        \n",
    "        # Salvare i risultati\n",
    "        results.append({\n",
    "            **param_dict,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_mse\": test_mse\n",
    "        })\n",
    "    \n",
    "    # Salvare i risultati in un file CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"TestsData/grid_search_results2.csv\", index=False)\n",
    "    print(\"Grid Search completata. Risultati salvati in grid_search_results.csv\")\n",
    "\n",
    "# Definizione della griglia dei parametri\n",
    "param_grid = {\n",
    "    \"n_centers\": [10, 15, 20],  # Numero di centri\n",
    "    \"gamma\": [0.001, 0.01, 0.1],  # Gamma per RBF\n",
    "    \"optimizer\": [optimizers.Adam, optimizers.SGD],  # Ottimizzatori\n",
    "    \"learning_rate\": [0.001, 0.01],  # Learning rate\n",
    "    \"batch_size\": [32, 64, 128]  # Diverse batch size\n",
    "}\n",
    "\n",
    "# Eseguire la Grid Search\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe962f-5759-451e-a4d0-5d71a5bb71c4",
   "metadata": {},
   "source": [
    "### Analisi results2:\n",
    "- Gamma: 0.1 è perfetto\n",
    "- Adam è il migliore nei primi 16\n",
    "- Centri alti sono meglio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf9d8e-8e44-480a-95e3-67a16698a494",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812ddc0-e76a-4904-9600-40843cf498b6",
   "metadata": {},
   "source": [
    "- Fissiamo Adam\n",
    "- Fissiamo gamma a 0.1\n",
    "- centri: tolgo 10, metto 25\n",
    "- learning rate: tolgo 0.001 e metto 0.05 e 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e54eac18-d05d-4672-aab7-bd37143a8807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.3531\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.3531\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.3354\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.3354\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.4953\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.4953\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1824\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.1824\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1749\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1749\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.2495\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.2495\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.1566\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.1566\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1228\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.1228\n",
      "Testing parameters: {'n_centers': 15, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.1792\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1792\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.2141\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.2141\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.3017\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Test MSE: 0.3017\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.4373\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.4373\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1724\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1724\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1793\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1793\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.1816\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1816\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.1669\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.1669\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1434\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Test MSE: 0.1434\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.1387\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.1387\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 32}\n",
      "Test Loss: 0.2425\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.2425\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 64}\n",
      "Test Loss: 0.2424\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.2424\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.01, 'batch_size': 128}\n",
      "Test Loss: 0.3434\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.3434\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1700\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Test MSE: 0.1700\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1422\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1422\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.1455\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1455\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.0859\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.0859\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1536\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1536\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.1339\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Test MSE: 0.1339\n",
      "Grid Search completata. Risultati salvati in grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Funzione per eseguire la Grid Search\n",
    "def grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid, epochs=50):\n",
    "    results = []\n",
    "    \n",
    "    for params in product(*param_grid.values()):\n",
    "        # Costruire il modello con i parametri correnti\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Testing parameters: {param_dict}\")\n",
    "        \n",
    "        model = build_rbf_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=y_train.shape[1],\n",
    "            n_centers=param_dict[\"n_centers\"],\n",
    "            gamma=param_dict[\"gamma\"]\n",
    "        )\n",
    "        \n",
    "        # Configurare l'ottimizzatore\n",
    "        optimizer = param_dict[\"optimizer\"](learning_rate=param_dict[\"learning_rate\"])\n",
    "        \n",
    "        # Passare il valore di batch_size\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        \n",
    "        # Compilare e allenare il modello\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Ottenere i valori di loss per train e validation\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Valutare il modello sui dati di test\n",
    "        test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Calcolare l'MSE sui dati di test\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        \n",
    "        # Salvare i risultati\n",
    "        results.append({\n",
    "            **param_dict,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_mse\": test_mse\n",
    "        })\n",
    "    \n",
    "    # Salvare i risultati in un file CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"TestsData/grid_search_results3.csv\", index=False)\n",
    "    print(\"Grid Search completata. Risultati salvati in grid_search_results.csv\")\n",
    "\n",
    "# Definizione della griglia dei parametri\n",
    "param_grid = {\n",
    "    \"n_centers\": [15, 20, 25],  # Numero di centri\n",
    "    \"gamma\": [0.1],  # Gamma per RBF\n",
    "    \"optimizer\": [optimizers.Adam],  # Ottimizzatori\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],  # Learning rate\n",
    "    \"batch_size\": [32, 64, 128]  # Diverse batch size\n",
    "}\n",
    "\n",
    "# Eseguire la Grid Search\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784515b-79f6-4d57-9a90-34cde8a61334",
   "metadata": {},
   "source": [
    "### Analisi results3:\n",
    "- Learning rate: 0.01 non più competitivo, 0.1 domina\n",
    "- Centri: preferisce gli alti (20-25)\n",
    "- Batch size sembra non avere correlazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f2eef-6ec0-4f9c-90ba-bfee73095e51",
   "metadata": {},
   "source": [
    "## Test 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c29093-2225-4969-8e08-21ad2e464b42",
   "metadata": {},
   "source": [
    "- Learning rate: togliamo 0.01, aggiungiamo 0.5 e 1.0\n",
    "- centri: togliamo 15, mettiamo 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb0a43d-cbc5-4dae-a77f-d196ed1cfcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1134\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1134\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1807\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1807\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.2546\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x165586a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/stepWARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x165586a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Test MSE: 0.2546\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.1122\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1122\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1424\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.1424\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.2175\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.2175\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 32}\n",
      "Test Loss: 0.0802\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.0802\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 64}\n",
      "Test Loss: 0.1253\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1253\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 128}\n",
      "Test Loss: 0.1195\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1195\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 32}\n",
      "Test Loss: 0.1168\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1168\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 64}\n",
      "Test Loss: 0.1328\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1328\n",
      "Testing parameters: {'n_centers': 20, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 128}\n",
      "Test Loss: 0.1005\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1005\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1347\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.1347\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1293\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1293\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.1946\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1946\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.1136\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1136\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1175\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1175\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.1530\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.1530\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 32}\n",
      "Test Loss: 0.0921\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0921\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 64}\n",
      "Test Loss: 0.0848\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.0848\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 128}\n",
      "Test Loss: 0.1052\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1052\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 32}\n",
      "Test Loss: 0.0767\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0767\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 64}\n",
      "Test Loss: 0.1083\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1083\n",
      "Testing parameters: {'n_centers': 25, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 128}\n",
      "Test Loss: 0.1278\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1278\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 32}\n",
      "Test Loss: 0.1619\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.1619\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 64}\n",
      "Test Loss: 0.1407\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1407\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.05, 'batch_size': 128}\n",
      "Test Loss: 0.1493\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1493\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 32}\n",
      "Test Loss: 0.1069\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.1069\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 64}\n",
      "Test Loss: 0.1035\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1035\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.1, 'batch_size': 128}\n",
      "Test Loss: 0.0954\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0954\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 32}\n",
      "Test Loss: 0.0794\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0794\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 64}\n",
      "Test Loss: 0.0860\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0860\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 0.5, 'batch_size': 128}\n",
      "Test Loss: 0.0793\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Test MSE: 0.0793\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 32}\n",
      "Test Loss: 0.1306\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Test MSE: 0.1306\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 64}\n",
      "Test Loss: 0.1489\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.1489\n",
      "Testing parameters: {'n_centers': 30, 'gamma': 0.1, 'optimizer': <class 'keras.src.optimizers.adam.Adam'>, 'learning_rate': 1.0, 'batch_size': 128}\n",
      "Test Loss: 0.0882\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Test MSE: 0.0882\n",
      "Grid Search completata. Risultati salvati in grid_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Funzione per eseguire la Grid Search\n",
    "def grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid, epochs=50):\n",
    "    results = []\n",
    "    \n",
    "    for params in product(*param_grid.values()):\n",
    "        # Costruire il modello con i parametri correnti\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Testing parameters: {param_dict}\")\n",
    "        \n",
    "        model = build_rbf_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            output_dim=y_train.shape[1],\n",
    "            n_centers=param_dict[\"n_centers\"],\n",
    "            gamma=param_dict[\"gamma\"]\n",
    "        )\n",
    "        \n",
    "        # Configurare l'ottimizzatore\n",
    "        optimizer = param_dict[\"optimizer\"](learning_rate=param_dict[\"learning_rate\"])\n",
    "        \n",
    "        # Passare il valore di batch_size\n",
    "        batch_size = param_dict[\"batch_size\"]\n",
    "        \n",
    "        # Compilare e allenare il modello\n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        # Ottenere i valori di loss per train e validation\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        # Valutare il modello sui dati di test\n",
    "        test_loss, test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Calcolare l'MSE sui dati di test\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        \n",
    "        # Salvare i risultati\n",
    "        results.append({\n",
    "            **param_dict,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_mse\": test_mse\n",
    "        })\n",
    "    \n",
    "    # Salvare i risultati in un file CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"TestsData/grid_search_results4.csv\", index=False)\n",
    "    print(\"Grid Search completata. Risultati salvati in grid_search_results.csv\")\n",
    "\n",
    "# Definizione della griglia dei parametri\n",
    "param_grid = {\n",
    "    \"n_centers\": [20, 25, 30],  # Numero di centri\n",
    "    \"gamma\": [0.1],  # Gamma per RBF\n",
    "    \"optimizer\": [optimizers.Adam],  # Ottimizzatori\n",
    "    \"learning_rate\": [0.05, 0.1, 0.5, 1.0],  # Learning rate\n",
    "    \"batch_size\": [32, 64, 128]  # Diverse batch size\n",
    "}\n",
    "\n",
    "# Eseguire la Grid Search\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search(X_train, y_train, X_val, y_val, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dcdd44e-8ad3-4048-ba03-06230600312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 01s]\n",
      "val_loss: 1.1532440185546875\n",
      "\n",
      "Best val_loss So Far: 1.1532440185546875\n",
      "Total elapsed time: 00h 00m 01s\n",
      "Migliori parametri: {'n_centers': 25, 'gamma': 0.1, 'learning_rate': 0.5, 'batch_size': 75, 'tuner/epochs': 2, 'tuner/initial_epoch': 0, 'tuner/bracket': 3, 'tuner/round': 0}\n",
      "Epoch 1/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 1.0655 - mse: 1.0655Epoch 1: Loss=1.9997094869613647, Val_Loss=0.9290977716445923\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 1.6883 - mse: 1.6883 - val_loss: 0.9291 - val_mse: 0.9291\n",
      "Epoch 2/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7749 - mse: 0.7749Epoch 2: Loss=0.92844158411026, Val_Loss=1.5163317918777466\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.8773 - mse: 0.8773 - val_loss: 1.5163 - val_mse: 1.5163\n",
      "Epoch 3/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.2933 - mse: 1.2933Epoch 3: Loss=1.272133708000183, Val_Loss=0.813093900680542\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.2792 - mse: 1.2792 - val_loss: 0.8131 - val_mse: 0.8131\n",
      "Epoch 4/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6162 - mse: 0.6162Epoch 4: Loss=0.6178736686706543, Val_Loss=1.048721432685852\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.6173 - mse: 0.6173 - val_loss: 1.0487 - val_mse: 1.0487\n",
      "Epoch 5/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.8515 - mse: 0.8515Epoch 5: Loss=0.8391577005386353, Val_Loss=0.7332442402839661\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8433 - mse: 0.8433 - val_loss: 0.7332 - val_mse: 0.7332\n",
      "Epoch 6/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6244 - mse: 0.6244Epoch 6: Loss=0.5620254278182983, Val_Loss=0.5708764791488647\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5828 - mse: 0.5828 - val_loss: 0.5709 - val_mse: 0.5709\n",
      "Epoch 7/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5124 - mse: 0.5124Epoch 7: Loss=0.4442180395126343, Val_Loss=0.5882158279418945\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4669 - mse: 0.4669 - val_loss: 0.5882 - val_mse: 0.5882\n",
      "Epoch 8/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4150 - mse: 0.4150Epoch 8: Loss=0.48677918314933777, Val_Loss=0.5136117339134216\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.4629 - mse: 0.4629 - val_loss: 0.5136 - val_mse: 0.5136\n",
      "Epoch 9/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4019 - mse: 0.4019Epoch 9: Loss=0.3335566520690918, Val_Loss=0.3731945753097534\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3563 - mse: 0.3563 - val_loss: 0.3732 - val_mse: 0.3732\n",
      "Epoch 10/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2450 - mse: 0.2450Epoch 10: Loss=0.3084568679332733, Val_Loss=0.4868714511394501\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2873 - mse: 0.2873 - val_loss: 0.4869 - val_mse: 0.4869\n",
      "Epoch 11/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3770 - mse: 0.3770Epoch 11: Loss=0.31930360198020935, Val_Loss=0.2589314877986908\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3385 - mse: 0.3385 - val_loss: 0.2589 - val_mse: 0.2589\n",
      "Epoch 12/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1858 - mse: 0.1858Epoch 12: Loss=0.19359801709651947, Val_Loss=0.33029866218566895\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1910 - mse: 0.1910 - val_loss: 0.3303 - val_mse: 0.3303\n",
      "Epoch 13/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2615 - mse: 0.2615Epoch 13: Loss=0.26437750458717346, Val_Loss=0.29027774930000305\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.2634 - mse: 0.2634 - val_loss: 0.2903 - val_mse: 0.2903\n",
      "Epoch 14/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1752 - mse: 0.1752Epoch 14: Loss=0.19242174923419952, Val_Loss=0.2534332275390625\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1867 - mse: 0.1867 - val_loss: 0.2534 - val_mse: 0.2534\n",
      "Epoch 15/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1607 - mse: 0.1607Epoch 15: Loss=0.19163550436496735, Val_Loss=0.2719319462776184\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1813 - mse: 0.1813 - val_loss: 0.2719 - val_mse: 0.2719\n",
      "Epoch 16/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1889 - mse: 0.1889Epoch 16: Loss=0.20463965833187103, Val_Loss=0.22848094999790192\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1994 - mse: 0.1994 - val_loss: 0.2285 - val_mse: 0.2285\n",
      "Epoch 17/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1594 - mse: 0.1594Epoch 17: Loss=0.16294899582862854, Val_Loss=0.2504488527774811\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1618 - mse: 0.1618 - val_loss: 0.2504 - val_mse: 0.2504\n",
      "Epoch 18/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1781 - mse: 0.1781Epoch 18: Loss=0.17433302104473114, Val_Loss=0.20463737845420837\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1756 - mse: 0.1756 - val_loss: 0.2046 - val_mse: 0.2046\n",
      "Epoch 19/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1724 - mse: 0.1724Epoch 19: Loss=0.1574104279279709, Val_Loss=0.18498487770557404\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1624 - mse: 0.1624 - val_loss: 0.1850 - val_mse: 0.1850\n",
      "Epoch 20/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1571 - mse: 0.1571Epoch 20: Loss=0.14314882457256317, Val_Loss=0.2337270975112915\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1478 - mse: 0.1478 - val_loss: 0.2337 - val_mse: 0.2337\n",
      "Epoch 21/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1549 - mse: 0.1549Epoch 21: Loss=0.1484527289867401, Val_Loss=0.19889897108078003\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1506 - mse: 0.1506 - val_loss: 0.1989 - val_mse: 0.1989\n",
      "Epoch 22/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1102 - mse: 0.1102Epoch 22: Loss=0.13161669671535492, Val_Loss=0.1778349131345749\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1245 - mse: 0.1245 - val_loss: 0.1778 - val_mse: 0.1778\n",
      "Epoch 23/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1512 - mse: 0.1512Epoch 23: Loss=0.1344880908727646, Val_Loss=0.1819789707660675\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1400 - mse: 0.1400 - val_loss: 0.1820 - val_mse: 0.1820\n",
      "Epoch 24/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1085 - mse: 0.1085Epoch 24: Loss=0.1176411435008049, Val_Loss=0.18891531229019165\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1146 - mse: 0.1146 - val_loss: 0.1889 - val_mse: 0.1889\n",
      "Epoch 25/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1276 - mse: 0.1276Epoch 25: Loss=0.11987101286649704, Val_Loss=0.17616580426692963\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1225 - mse: 0.1225 - val_loss: 0.1762 - val_mse: 0.1762\n",
      "Epoch 26/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1186 - mse: 0.1186Epoch 26: Loss=0.1152709424495697, Val_Loss=0.16685855388641357\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1164 - mse: 0.1164 - val_loss: 0.1669 - val_mse: 0.1669\n",
      "Epoch 27/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1139 - mse: 0.1139Epoch 27: Loss=0.11126332730054855, Val_Loss=0.16837050020694733\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1121 - mse: 0.1121 - val_loss: 0.1684 - val_mse: 0.1684\n",
      "Epoch 28/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1026 - mse: 0.1026Epoch 28: Loss=0.11187243461608887, Val_Loss=0.15612947940826416\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1088 - mse: 0.1088 - val_loss: 0.1561 - val_mse: 0.1561\n",
      "Epoch 29/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0987 - mse: 0.0987Epoch 29: Loss=0.10871993750333786, Val_Loss=0.1644030511379242\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1054 - mse: 0.1054 - val_loss: 0.1644 - val_mse: 0.1644\n",
      "Epoch 30/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1068 - mse: 0.1068Epoch 30: Loss=0.10602893680334091, Val_Loss=0.14834338426589966\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1063 - mse: 0.1063 - val_loss: 0.1483 - val_mse: 0.1483\n",
      "Epoch 31/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0796 - mse: 0.0796Epoch 31: Loss=0.10257180035114288, Val_Loss=0.14551611244678497\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0949 - mse: 0.0949 - val_loss: 0.1455 - val_mse: 0.1455\n",
      "Epoch 32/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0868 - mse: 0.0868Epoch 32: Loss=0.10152284801006317, Val_Loss=0.1513391137123108\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.1513 - val_mse: 0.1513\n",
      "Epoch 33/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1289 - mse: 0.1289Epoch 33: Loss=0.09917055070400238, Val_Loss=0.14636486768722534\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1091 - mse: 0.1091 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 34/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1130 - mse: 0.1130Epoch 34: Loss=0.09756786376237869, Val_Loss=0.14620283246040344\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.1027 - mse: 0.1027 - val_loss: 0.1462 - val_mse: 0.1462\n",
      "Epoch 35/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1003 - mse: 0.1003Epoch 35: Loss=0.09657510370016098, Val_Loss=0.14642608165740967\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 36/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0825 - mse: 0.0825Epoch 36: Loss=0.0942239835858345, Val_Loss=0.14746180176734924\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0903 - mse: 0.0903 - val_loss: 0.1475 - val_mse: 0.1475\n",
      "Epoch 37/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0989 - mse: 0.0989Epoch 37: Loss=0.09235445410013199, Val_Loss=0.14016470313072205\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0945 - mse: 0.0945 - val_loss: 0.1402 - val_mse: 0.1402\n",
      "Epoch 38/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0818 - mse: 0.0818Epoch 38: Loss=0.09287203103303909, Val_Loss=0.14171770215034485\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1417 - val_mse: 0.1417\n",
      "Epoch 39/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0865 - mse: 0.0865Epoch 39: Loss=0.09006458520889282, Val_Loss=0.14561139047145844\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0889 - mse: 0.0889 - val_loss: 0.1456 - val_mse: 0.1456\n",
      "Epoch 40/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0882 - mse: 0.0882Epoch 40: Loss=0.08962932229042053, Val_Loss=0.13359424471855164\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0892 - mse: 0.0892 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 41/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0823 - mse: 0.0823Epoch 41: Loss=0.08880285918712616, Val_Loss=0.1340397149324417\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0866 - mse: 0.0866 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 42/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1032 - mse: 0.1032Epoch 42: Loss=0.08734701573848724, Val_Loss=0.14273834228515625\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0926 - mse: 0.0926 - val_loss: 0.1427 - val_mse: 0.1427\n",
      "Epoch 43/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0696 - mse: 0.0696Epoch 43: Loss=0.08684242516756058, Val_Loss=0.12968331575393677\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0811 - mse: 0.0811 - val_loss: 0.1297 - val_mse: 0.1297\n",
      "Epoch 44/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0752 - mse: 0.0752Epoch 44: Loss=0.08437362313270569, Val_Loss=0.13784463703632355\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0813 - mse: 0.0813 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 45/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0622 - mse: 0.0622Epoch 45: Loss=0.08421044796705246, Val_Loss=0.13229696452617645\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0769 - mse: 0.0769 - val_loss: 0.1323 - val_mse: 0.1323\n",
      "Epoch 46/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0664 - mse: 0.0664Epoch 46: Loss=0.08405700325965881, Val_Loss=0.1291266679763794\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0782 - mse: 0.0782 - val_loss: 0.1291 - val_mse: 0.1291\n",
      "Epoch 47/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0876 - mse: 0.0876Epoch 47: Loss=0.08223610371351242, Val_Loss=0.13359825313091278\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0840 - mse: 0.0840 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 48/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0654 - mse: 0.0654Epoch 48: Loss=0.0805187001824379, Val_Loss=0.12123946100473404\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0755 - mse: 0.0755 - val_loss: 0.1212 - val_mse: 0.1212\n",
      "Epoch 49/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0672 - mse: 0.0672Epoch 49: Loss=0.08143506199121475, Val_Loss=0.12400050461292267\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0767 - mse: 0.0767 - val_loss: 0.1240 - val_mse: 0.1240\n",
      "Epoch 50/50\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0781 - mse: 0.0781Epoch 50: Loss=0.07930944114923477, Val_Loss=0.12655110657215118\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0789 - mse: 0.0789 - val_loss: 0.1266 - val_mse: 0.1266\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABoGUlEQVR4nO3deXwU9eH/8dfsJtnNTQK5kHDJfd+neFQEUalULdgqakX9eYv0ROvVby3aasXb2qrU2gK24FVRgVYOCyogQRREjkAQEsKZkxy7O78/ZneTzbkbchHez8djHrM7OzP72Snfb95+TsM0TRMRERGRVszW0gUQERERqY8Ci4iIiLR6CiwiIiLS6imwiIiISKunwCIiIiKtngKLiIiItHoKLCIiItLqKbCIiIhIqxfW0gVoLB6Ph4MHDxIbG4thGC1dHBEREQmCaZoUFBTQsWNHbLba61HaTGA5ePAg6enpLV0MERERaYD9+/fTqVOnWj9vM4ElNjYWsH5wXFxcC5dGREREgpGfn096err/73ht2kxg8TUDxcXFKbCIiIicZurrzqFOtyIiItLqKbCIiIhIq6fAIiIiIq1em+nDIiIiDWeaJi6XC7fb3dJFkTbGbrcTFhZ2ylOOKLCIiJzhysrKyM7Opri4uKWLIm1UVFQUaWlpRERENPgeCiwiImcwj8dDZmYmdrudjh07EhERock3pdGYpklZWRmHDx8mMzOTnj171jk5XF0UWEREzmBlZWV4PB7S09OJiopq6eJIGxQZGUl4eDj79u2jrKwMp9PZoPuo062IiDT4v3pFgtEY/770L1RERERavZACy7x58xg5ciSxsbEkJyczbdo0duzYUe91q1evZvjw4TidTrp3785LL71U7ZwlS5bQr18/HA4H/fr146233gqlaCIiIqfs/PPPZ/bs2UGfv3fvXgzDICMjo8nKJJaQAsvq1au54447+PTTT1mxYgUul4tJkyZRVFRU6zWZmZlccsklTJgwgc2bN3Pfffdx9913s2TJEv8569evZ8aMGcycOZMtW7Ywc+ZMpk+fzmeffdbwXyYiIm2WYRh1bjfccEOD7rt06VL+7//+L+jz09PTyc7OZsCAAQ36vmApGIFhmqbZ0IsPHz5McnIyq1ev5txzz63xnF/+8pe8++67bN++3X/s1ltvZcuWLaxfvx6AGTNmkJ+fzwcffOA/5+KLLyYhIYGFCxcGVZb8/Hzi4+PJy8vTWkIiIkEqKSkhMzOTbt26NbgzZEvIycnxv168eDEPPvhgQI1/ZGQk8fHx/vfl5eWEh4c3axkb0969e+nWrRubN29myJAhLV2ckNX17yzYv9+n1IclLy8PgMTExFrPWb9+PZMmTQo4NnnyZDZu3Eh5eXmd56xbt67W+5aWlpKfnx+wNYVXP8nkwXe+4ttDBU1yfxERCV1qaqp/i4+PxzAM//uSkhLatWvHm2++yfnnn4/T6eSNN97g6NGj/OhHP6JTp05ERUUxcODAav9RXLVJqGvXrvzud7/jxhtvJDY2ls6dO/Pyyy/7P69a87Fq1SoMw+A///kPI0aMICoqinHjxlXrPvHb3/6W5ORkYmNjuemmm/jVr351SkGktLSUu+++m+TkZJxOJ+eccw4bNmzwf378+HGuueYakpKSiIyMpGfPnrz22muANVLszjvvJC0tDafTSdeuXZk3b16Dy9JUGhxYTNNkzpw5nHPOOXVWheXk5JCSkhJwLCUlBZfLxZEjR+o8p3KCrmrevHnEx8f7t/T09Ib+lDq99+VBXl+/j8wjtTd7iYi0JaZpUlzmapHtFCr9q/nlL3/J3Xffzfbt25k8eTIlJSUMHz6cf//733z11VfccsstzJw5s97uB08++SQjRoxg8+bN3H777dx222188803dV5z//338+STT7Jx40bCwsK48cYb/Z/9/e9/59FHH+Xxxx9n06ZNdO7cmRdffPGUfusvfvELlixZwl//+le++OILevToweTJkzl27BgADzzwANu2beODDz5g+/btvPjii3To0AGAZ555hnfffZc333yTHTt28MYbb9C1a9dTKk9TaPA8LHfeeSdffvkln3zySb3nVp2EyPcPsvLxms6pa/KiuXPnMmfOHP/7/Pz8JgktMQ7rERWXuRr93iIirdHJcjf9HvyoRb57228mExXROFOEzZ49myuuuCLg2M9+9jP/67vuuosPP/yQf/7zn4wePbrW+1xyySXcfvvtgBWCnnrqKVatWkWfPn1qvebRRx/lvPPOA+BXv/oVl156KSUlJTidTp599llmzZrFT37yEwAefPBBli9fTmFhYYN+Z1FRES+++CILFixgypQpAPz5z39mxYoVvPLKK/z85z8nKyuLoUOHMmLECICAQJKVlUXPnj0555xzMAyDLl26NKgcTa1BNSx33XUX7777Lh9//DGdOnWq89zU1NRqNSW5ubmEhYXRvn37Os+pWutSmcPhIC4uLmBrClERdgAKS7W+hojI6cT3x9nH7Xbz6KOPMmjQINq3b09MTAzLly8nKyurzvsMGjTI/9rX9JSbmxv0NWlpaQD+a3bs2MGoUaMCzq/6PhS7d++mvLyc8ePH+4+Fh4czatQof//R2267jUWLFjFkyBB+8YtfBHS5uOGGG8jIyKB3797cfffdLF++vMFlaUohxVjTNLnrrrt46623WLVqFd26dav3mrFjx/Lee+8FHFu+fDkjRozwd4AaO3YsK1as4N577w04Z9y4caEUr0lE+2pYSlXDIiJnhshwO9t+M7nFvruxREdHB7x/8skneeqpp5g/fz4DBw4kOjqa2bNnU1ZWVud9qnbWNQwDj8cT9DW+1oLK19TW8tAQNbVa+I77jk2ZMoV9+/bx/vvvs3LlSi688ELuuOMOnnjiCYYNG0ZmZiYffPABK1euZPr06UycOJF//etfDS5TUwiphuWOO+7gjTfe4B//+AexsbHk5OSQk5PDyZMn/efMnTuX6667zv/+1ltvZd++fcyZM4ft27fz6quv8sorrwRUy91zzz0sX76cxx9/nG+++YbHH3+clStXhjQWvqlEe6smixRYROQMYRgGURFhLbI15TpGa9eu5fLLL+faa69l8ODBdO/enZ07dzbZ99Wmd+/efP755wHHNm7c2OD79ejRg4iIiIAuGuXl5WzcuJG+ffv6jyUlJXHDDTfwxhtvMH/+/IDOw3FxccyYMYM///nPLF68mCVLlvj7v7QWIdWw+DoFnX/++QHHX3vtNf+Y9+zs7IDqtW7durFs2TLuvfdenn/+eTp27MgzzzzDlVde6T9n3LhxLFq0iF//+tc88MADnH322SxevLjONsXm4qthUZOQiMjprUePHixZsoR169aRkJDAH//4R3JycgL+qDeHu+66i5tvvpkRI0Ywbtw4Fi9ezJdffkn37t3rvbamyVr79evHbbfdxs9//nMSExPp3Lkzv//97ykuLmbWrFmA1U9m+PDh9O/fn9LSUv7973/7f/dTTz1FWloaQ4YMwWaz8c9//pPU1FTatWvXqL/7VIXcJFSfBQsWVDt23nnn8cUXX9R53VVXXcVVV10VSnGaRbS3D4s63YqInN4eeOABMjMzmTx5MlFRUdxyyy1MmzbNP0VHc7nmmmvYs2cPP/vZzygpKWH69OnccMMN1WpdanL11VdXO5aZmcljjz2Gx+Nh5syZFBQUMGLECD766CMSEhIAiIiIYO7cuezdu5fIyEgmTJjAokWLAIiJieHxxx9n586d2O12Ro4cybJly1rd+lKnNHFca9JUE8e9+kkmv/n3Ni4blMZzPx7WaPcVEWkNTteJ49qaiy66iNTUVP72t7+1dFGaRGNMHNc4Y8fasGiHr4ZFTUIiInLqiouLeemll5g8eTJ2u52FCxeycuVKVqxY0dJFa9UUWOpR0YdFTUIiInLqDMNg2bJl/Pa3v6W0tJTevXuzZMkSJk6c2NJFa9UUWOoRrYnjRESkEUVGRrJy5cqWLsZpp3X1qGmFKoY1q0lIRESkpSiw1MPXh0VNQiIiIi1HgaUevhoWzXQrIiLSchRY6uHrw1JU5sbjaRMjwEVERE47Ciz18DUJgbWCqYiIiDQ/BZZ6RIbbsXmXttB6QiIiIi1DgaUehmFUjBTS5HEiIm3K+eefH7DQbteuXZk/f36d1xiGwdtvv33K391Y9zlTKLAEIcrbLKQaFhGR1mHq1Km1TrS2fv16DMOodw27mmzYsIFbbrnlVIsX4OGHH2bIkCHVjmdnZzNlypRG/a6qFixY0OoWMWwoBZYgaLZbEZHWZdasWfz3v/9l37591T579dVXGTJkCMOGhb7+W1JSElFRUY1RxHqlpqbicDia5bvaAgWWIPiHNmu2WxGRVuGyyy4jOTmZBQsWBBwvLi5m8eLFzJo1i6NHj/KjH/2ITp06ERUVxcCBA1m4cGGd963aJLRz507OPfdcnE4n/fr1q3G9n1/+8pf06tWLqKgounfvzgMPPEB5eTlg1XA88sgjbNmyBcMwMAzDX+aqTUJbt27le9/7HpGRkbRv355bbrmFwsJC/+c33HAD06ZN44knniAtLY327dtzxx13+L+rIbKysrj88suJiYkhLi6O6dOnc+jQIf/nW7Zs4YILLiA2Npa4uDiGDx/Oxo0bAdi3bx9Tp04lISGB6Oho+vfvz7Jlyxpclvpoav4gVEwepz4sInIGME0oL26Z7w6PAsOo97SwsDCuu+46FixYwIMPPojhveaf//wnZWVlXHPNNRQXFzN8+HB++ctfEhcXx/vvv8/MmTPp3r07o0ePrvc7PB4PV1xxBR06dODTTz8lPz8/oL+LT2xsLAsWLKBjx45s3bqVm2++mdjYWH7xi18wY8YMvvrqKz788EP/dPzx8fHV7lFcXMzFF1/MmDFj2LBhA7m5udx0003ceeedAaHs448/Ji0tjY8//phdu3YxY8YMhgwZws0331zv76nKNE2mTZtGdHQ0q1evxuVycfvttzNjxgxWrVoFwDXXXMPQoUN58cUXsdvtZGRkEB4eDsAdd9xBWVkZa9asITo6mm3bthETExNyOYKlwBIETR4nImeU8mL4XceW+e77DkJEdFCn3njjjfzhD39g1apVXHDBBYDVHHTFFVeQkJBAQkICP/vZz/zn33XXXXz44Yf885//DCqwrFy5ku3bt7N37146deoEwO9+97tq/U5+/etf+1937dqVn/70pyxevJhf/OIXREZGEhMTQ1hYGKmpqbV+19///ndOnjzJ66+/TnS09fufe+45pk6dyuOPP05KSgoACQkJPPfcc9jtdvr06cOll17Kf/7znwYFlpUrV/Lll1+SmZlJeno6AH/729/o378/GzZsYOTIkWRlZfHzn/+cPn36ANCzZ0//9VlZWVx55ZUMHDgQgO7du4dchlCoSSgI6sMiItL69OnTh3HjxvHqq68CsHv3btauXcuNN94IgNvt5tFHH2XQoEG0b9+emJgYli9fTlZWVlD33759O507d/aHFYCxY8dWO+9f//oX55xzDqmpqcTExPDAAw8E/R2Vv2vw4MH+sAIwfvx4PB4PO3bs8B/r378/dnvF/GBpaWnk5uaG9F2VvzM9Pd0fVgD69etHu3bt2L59OwBz5szhpptuYuLEiTz22GPs3r3bf+7dd9/Nb3/7W8aPH89DDz3El19+2aByBEs1LEGoWLFZTUIicgYIj7JqOlrqu0Mwa9Ys7rzzTp5//nlee+01unTpwoUXXgjAk08+yVNPPcX8+fMZOHAg0dHRzJ49m7KysqDubZrVZzc3qjRXffrpp1x99dU88sgjTJ48mfj4eBYtWsSTTz4Z0u8wTbPavWv6Tl9zTOXPPB5PSN9V33dWPv7www/z4x//mPfff58PPviAhx56iEWLFvGDH/yAm266icmTJ/P++++zfPly5s2bx5NPPsldd93VoPLURzUsQYiO0LBmETmDGIbVLNMSWxD9VyqbPn06drudf/zjH/z1r3/lJz/5if+P7dq1a7n88su59tprGTx4MN27d2fnzp1B37tfv35kZWVx8GBFeFu/fn3AOf/73//o0qUL999/PyNGjKBnz57VRi5FRETgdtf9H7z9+vUjIyODoqKigHvbbDZ69eoVdJlD4ft9+/fv9x/btm0beXl59O3b13+sV69e3HvvvSxfvpwrrriC1157zf9Zeno6t956K0uXLuWnP/0pf/7zn5ukrKDAEhQ1CYmItE4xMTHMmDGD++67j4MHD3LDDTf4P+vRowcrVqxg3bp1bN++nf/3//4fOTk5Qd974sSJ9O7dm+uuu44tW7awdu1a7r///oBzevToQVZWFosWLWL37t0888wzvPXWWwHndO3alczMTDIyMjhy5AilpaXVvuuaa67B6XRy/fXX89VXX/Hxxx9z1113MXPmTH//lYZyu91kZGQEbNu2bWPixIkMGjSIa665hi+++ILPP/+c6667jvPOO48RI0Zw8uRJ7rzzTlatWsW+ffv43//+x4YNG/xhZvbs2Xz00UdkZmbyxRdf8N///jcg6DQ2BZYg+EYJqUlIRKT1mTVrFsePH2fixIl07tzZf/yBBx5g2LBhTJ48mfPPP5/U1FSmTZsW9H1tNhtvvfUWpaWljBo1iptuuolHH3004JzLL7+ce++9lzvvvJMhQ4awbt06HnjggYBzrrzySi6++GIuuOACkpKSahxaHRUVxUcffcSxY8cYOXIkV111FRdeeCHPPfdcaA+jBoWFhQwdOjRgu+SSS/zDqhMSEjj33HOZOHEi3bt3Z/HixQDY7XaOHj3KddddR69evZg+fTpTpkzhkUceAawgdMcdd9C3b18uvvhievfuzQsvvHDK5a2NYdbUSHcays/PJz4+nry8POLi4hr13n//bB/3v/UVF/VL4c/XjWjUe4uItKSSkhIyMzPp1q0bTqezpYsjbVRd/86C/futGpYgaOI4ERGRlqXAEoSKPixqEhIREWkJCixB8PdhUadbERGRFqHAEgRfk5CGNYuIiLQMBZYgaFiziIhIy1JgCULlYc1tZFCViEgA/f82aUqN8e9LgSUIvhoWl8ek1NWwKZBFRFoj31TvxcUttDqznBF8/76qLi0QCq0lFISo8IqFporL3DgrvRcROZ3Z7XbatWvnX0AvKiqq1jVtREJlmibFxcXk5ubSrl27gIUbQ6XAEoQwuw1nuI2Scg9FpS4SoyNaukgiIo0mNTUVoMGr/orUp127dv5/Zw2lwBKkGEcYJeVlFGnyOBFpYwzDIC0tjeTkZMrLy1u6ONLGhIeHn1LNio8CS5CiIsKAMg1tFpE2y263N8ofFpGmEHKn2zVr1jB16lQ6duzoXzipLjfccAOGYVTb+vfv7z9nwYIFNZ5TUlIS8g9qKr6Ot0Wa7VZERKTZhRxYioqKGDx4cNArSD799NNkZ2f7t/3795OYmMgPf/jDgPPi4uICzsvOzm5VC3FFR1j/1aEaFhERkeYXcpPQlClTmDJlStDnx8fHEx8f73//9ttvc/z4cX7yk58EnGcYxil3yGlKmjxORESk5TT7PCyvvPIKEydOpEuXLgHHCwsL6dKlC506deKyyy5j8+bNdd6ntLSU/Pz8gK0pVZ48TkRERJpXswaW7OxsPvjgA2666aaA43369GHBggW8++67LFy4EKfTyfjx49m5c2et95o3b56/9iY+Pp709PQmLbtvPSHVsIiIiDS/Zg0sCxYsoF27dkybNi3g+JgxY7j22msZPHgwEyZM4M0336RXr148++yztd5r7ty55OXl+bf9+/c3adl9TULFGtYsIiLS7JptWLNpmrz66qvMnDmTiIi6J16z2WyMHDmyzhoWh8OBw+Fo7GLWytckpFFCIiIiza/ZalhWr17Nrl27mDVrVr3nmqZJRkYGaWlpzVCy4FQMa1YNi4iISHMLuYalsLCQXbt2+d9nZmaSkZFBYmIinTt3Zu7cuRw4cIDXX3894LpXXnmF0aNHM2DAgGr3fOSRRxgzZgw9e/YkPz+fZ555hoyMDJ5//vkG/KSm4evDopluRUREml/IgWXjxo1ccMEF/vdz5swB4Prrr2fBggVkZ2eTlZUVcE1eXh5Llizh6aefrvGeJ06c4JZbbiEnJ4f4+HiGDh3KmjVrGDVqVKjFazIVw5rVJCQiItLcDNM0zZYuRGPIz88nPj6evLw84uLiGv3+H2zN5ra/f8GILgn867ZxjX5/ERGRM1Gwf7+bfR6W05UmjhMREWk5CixBqhjWrCYhERGR5qbAEqSKYc2qYREREWluCixB0ighERGRlqPAEiRfk1BJuQeX29PCpRERETmzKLAEydckBFCkfiwiIiLNSoElSBF2G2E2A9B6QiIiIs1NgSVIhmFoen4REZEWosASghh/YFGTkIiISHNSYAlBVISGNouIiLQEBZYQ+JuE1OlWRESkWSmwhECTx4mIiLQMBZYQ+CaP03pCIiIizUuBJQQV6wkpsIiIiDQnBZYQ+JqECjVKSEREpFkpsITAX8OiJiEREZFmpcASAi2AKCIi0jIUWEIQrYnjREREWoQCSwiiNXGciIhIi1BgCYGvhkXDmkVERJqXAksIfKOEijXTrYiISLNSYAmBv9OtalhERESalQJLCCrWElJgERERaU4KLCHQKCEREZGWocASAv/ih2UuTNNs4dKIiIicORRYQuDrw2KacLJctSwiIiLNRYElBJHhdgzDeq2hzSIiIs1HgSUENptBVLh3aLP6sYiIiDQbBZYQafI4ERGR5qfAEiL/is2aPE5ERKTZKLCEyD9SSDUsIiIizUaBJUT+2W41eZyIiEizUWAJUcXkcQosIiIizSXkwLJmzRqmTp1Kx44dMQyDt99+u87zV61ahWEY1bZvvvkm4LwlS5bQr18/HA4H/fr146233gq1aM2ixtluv1kGT/aBPatbqFQiIiJtW8iBpaioiMGDB/Pcc8+FdN2OHTvIzs72bz179vR/tn79embMmMHMmTPZsmULM2fOZPr06Xz22WehFq/JRUfU0Iflm/ehIBt2Lm+hUomIiLRtYaFeMGXKFKZMmRLyFyUnJ9OuXbsaP5s/fz4XXXQRc+fOBWDu3LmsXr2a+fPns3DhwpC/qyn5hzVX7sNSlGvtTx5vgRKJiIi0fc3Wh2Xo0KGkpaVx4YUX8vHHHwd8tn79eiZNmhRwbPLkyaxbt67W+5WWlpKfnx+wNQdfDUvAxHGF3sBSfKxZyiAiInKmafLAkpaWxssvv8ySJUtYunQpvXv35sILL2TNmjX+c3JyckhJSQm4LiUlhZycnFrvO2/ePOLj4/1benp6k/2GymrsdFt02NqfVGARERFpCiE3CYWqd+/e9O7d2/9+7Nix7N+/nyeeeIJzzz3Xf9zwLdLjZZpmtWOVzZ07lzlz5vjf5+fnN0to8QcWX5OQaVYEluKjTf79IiIiZ6IWGdY8ZswYdu7c6X+fmpparTYlNze3Wq1LZQ6Hg7i4uICtOVRMHOdtEio5Ae4y67WahERERJpEiwSWzZs3k5aW5n8/duxYVqxYEXDO8uXLGTduXHMXrV7VJo4rPFzxYckJ8Hiav1AiIiJtXMhNQoWFhezatcv/PjMzk4yMDBITE+ncuTNz587lwIEDvP7664A1Aqhr167079+fsrIy3njjDZYsWcKSJUv897jnnns499xzefzxx7n88st55513WLlyJZ988kkj/MTGVa0Pi2+EEIDpsUJLVGLzF0xERKQNCzmwbNy4kQsuuMD/3teP5Prrr2fBggVkZ2eTlZXl/7ysrIyf/exnHDhwgMjISPr378/777/PJZdc4j9n3LhxLFq0iF//+tc88MADnH322SxevJjRo0efym9rEtUmjivMDTzh5HEFFhERkUZmmKZptnQhGkN+fj7x8fHk5eU1aX+WnYcKuOipNbSLCifjwUnw2Z/gg19UnDBrJaSPbLLvFxERaUuC/futtYRCVK1JqGoNi0YKiYiINDoFlhD5Aku526TM5QnswwKai0VERKQJKLCEyDfTLXhrWSqPEgINbRYREWkCCiwhCrPbcIRZj62ozFVRwxLvnbRONSwiIiKNToGlAQJGCvlmuU3yzuarGhYREZFGp8DSAL7ZbgtLyiuahJL6WHvVsIiIiDQ6BZYG8M12W1qcB66T1sEOvay9alhEREQanQJLA/iahFz5h6wD4dEQ38l6rcAiIiLS6BRYGsAXWDwF3g63MUkVs9uqSUhERKTRKbA0gH9os2+EUHQyRHoDS/ExaBuTB4uIiLQaCiwN4KthsRUdsQ7EJFfUsLhLoby4hUomIiLSNimwNICvhiXspHeEUHQSRMSALdx6r34sIiIijUqBpQF8NSzhJd51g2KSwTDUj0VERKSJKLA0gC+wOMu8TULRSdY+qr21Vw2LiIhIo1JgaQBfk1BUmTeYxCRbe3/HW63YLCIi0pgUWBrAV8MS4zruPeANLFEJ1v7k8RYolYiISNulwNIAvsAS5/YGk2o1LGoSEhERaUwKLA0Q7QjDSSmRpndafn8fFnW6FRERaQoKLA0QHWGng5FnvQlzgiPWeq0aFhERkSahwNIA0Y4wOpDvfeMd0gyqYREREWkiCiwNEB0RVlHDEpNU8YF/WLNGCYmIiDQmBZYGiHZUNAmZUR0qPlCTkIiISJNQYGkAq0nICizlkZUCi79JSMOaRUREGpMCSwM4wmwk26zAUuasoYalNB/c5S1QMhERkbZJgaUBDMMgxW51ui2JaF/xQWQ7wNsBV7UsIiIijUaBpYGSDSuwFIUnVhy02cEZb71WPxYREZFGo8DSQO29nW4LwxIDP9DQZhERkUanwNJAieYJAPLs7QI/0NBmERGRRqfA0hCuUmLMIgBO2NoFfqahzSIiIo1OgaUhig4DUGbaOeGJCfxMTUIiIiKNToGlIQpzAThKPEVl7sDPVMMiIiLS6BRYGsJbw3LEjKOozBX4WVSCta9Sw2KaJjsPFVDqqhJwREREpF4hB5Y1a9YwdepUOnbsiGEYvP3223Wev3TpUi666CKSkpKIi4tj7NixfPTRRwHnLFiwAMMwqm0lJSWhFq95eGtYjpjxFJVWCSz+GpbAeVjW7T7KRU+t4eF3v26OEoqIiLQpIQeWoqIiBg8ezHPPPRfU+WvWrOGiiy5i2bJlbNq0iQsuuICpU6eyefPmgPPi4uLIzs4O2JxOZ6jFax7+GpYamoR8o4Sq1LBs2Gu935FT0OTFExERaWvCQr1gypQpTJkyJejz58+fH/D+d7/7He+88w7vvfceQ4cO9R83DIPU1NRQi9MyfIGFGmpYfJ1uqwxr3n3YGlV0rKisyYsnIiLS1jR7HxaPx0NBQQGJiYETrhUWFtKlSxc6derEZZddVq0GplXxNwnF1dEkFFjDsudwIaDAIiIi0hDNHliefPJJioqKmD59uv9Ynz59WLBgAe+++y4LFy7E6XQyfvx4du7cWet9SktLyc/PD9iaTVHlPixVm4QqrdhsmoDV4TbziFXDkl/iotztabaiioiItAXNGlgWLlzIww8/zOLFi0lOTvYfHzNmDNdeey2DBw9mwoQJvPnmm/Tq1Ytnn3221nvNmzeP+Ph4/5aent4cP8FSWKlJqOooIV8Ni+mGEmv6/pz8Eoor9XU5rloWERGRkDRbYFm8eDGzZs3izTffZOLEiXWea7PZGDlyZJ01LHPnziUvL8+/7d+/v7GLXLuiOkYJhTshPMp67e14u8fbf8XnWLECi4iISChC7nTbEAsXLuTGG29k4cKFXHrppfWeb5omGRkZDBw4sNZzHA4HDoejMYsZHLfL3z/liBlPeNUmIbBqWcqLraHNiRX9V3yOFSqwiIiIhCLkwFJYWMiuXbv87zMzM8nIyCAxMZHOnTszd+5cDhw4wOuvvw5YYeW6667j6aefZsyYMeTk5AAQGRlJfHw8AI888ghjxoyhZ8+e5Ofn88wzz5CRkcHzzz/fGL+xcRUfAUxMw8ZxYomu2iQEVj+W/O/8I4V2q4ZFRETklITcJLRx40aGDh3qH5I8Z84chg4dyoMPPghAdnY2WVlZ/vP/9Kc/4XK5uOOOO0hLS/Nv99xzj/+cEydOcMstt9C3b18mTZrEgQMHWLNmDaNGjTrV39f4vCOEPJHt8WCjqNSF6e1c61dlPaHdVWtY1IdFREQkJCHXsJx//vnV/0BXsmDBgoD3q1atqveeTz31FE899VSoRWkZ3v4rxCTDMfCYUFLuITLCXnFOlaHNvj4svVJi+PZQIUfVJCQiIhISrSUUKu8IIVtMxSin6usJVdSwlJS7OZh3EoDhXazjx9UkJCIiEhIFllB5a1iMmGSivLUqdU0el3mkCNOEOGcYPZNjADiqJiEREZGQKLCEytuHhegkoh1Wi1rtk8cd8zcHdU+KoX1MBKB5WEREREKlwBIq7zpCxCQT7athqW3yuOJj/iHN3ZOiSYy2Aos63YqIiIRGgSVU/hqW5Eo1LFX7sHhXbC4+5h8hdHZSDAlRCiwiIiIN0SwTx7UpRUesfUwS0RG1NQklWPuTx9jjtpqEzk6KrmgSKi7DNE0Mw2iWIouIiJzuVMMSqqLKfVjq7nRrFgf2YfHVsJS7TfJLaphwTkRERGqkwBIKj6eihqVyk1Atw5oN10nKS4uxGdClfRTOcLu/34s63oqIiARPgSUUJ49ZqzADRHeo1CRUJbA44sBmfZZAAZ0SonCEWUElwdvxVkObRUREgqfAEgpfh9vIRLCHV6phqdKHxTAg0urHkmAUcnZStP+j9tEa2iwiIhIqBZZQVJ6WH2rvwwL+fiztjEK6J8X4D2tos4iISOgUWELhnZaf6CRrV9vEceAf2pxIAd0r1bD4moS0YrOIiEjwFFhCUbWGpbap+cHf8TbBKKB7h4oalvaqYREREQmZAksoKk0aB9Q+SghwO9sB0I7APiz+TrdasVlERCRoCiyh8E/LbzUJRdU2SgjIIxaA5LAikmId/uP+TrdqEhIREQmaAksoqtSwxNTRh+Ww26pVSXeWBMxo65s8TsOaRUREgqfAEoraRgnV0CR0oDQSgJSw4oDjWrFZREQkdAosoah1lFD1wJJ50glAoq0g4HhitNU8pE63IiIiwVNgCZZpVurDUrXTbfUmoV0FVk1KrKdKYPE2CRWWuih11TAcWkRERKpRYAnWyePgKbde+2pYvMOay1weyt0e/6mmabLthPWZszwv4DZxkWHYbVafluNF5U1dahERkTZBgSVYvkUPHfEQZjXr+EYJQWCz0NGiMr4rsfqw2MvywF3xmWEYlTreljZ1qUVERNoEBZZg+TvcJvkPRYTZiLBbj7Bys9Cew0WcoGKyOEpOBNyqYj0h1bCIiIgEQ4ElWFWGNPvUtJ7Q7sOFuLFTZHgnjCs+FnBNQnQ4oBoWERGRYCmwBKvKpHE+NY0U2nO4EIDS8HjrwMnAwNLeO1JIQ5tFRESCo8ASrNpqWCKqTx6353ARAB7vis0UHw24Ris2i4iIhEaBJVhVJo3zqWnyuD1HrMBii7ZWbK7eJKQVm0VEREKhwBKsKpPG+VRtEipzecg6Zs1uGxnnPbdak5BqWEREREKhwBKs2mpYIgInj8s6VozbYxIdYccZ38E6qZYalhpXbC4tBJeCjIiISGUKLMHy17AEBpaoKqOEdns73HZLisaI8jYJ1VLDUm3F5qKj8PQg+PuVjVlyERGR054CSzBMs8Z5WKDyis1WYPF1uO3eIQYiE6yTqtSw1NrpNnO11UE3cw2UFTXmLxARETmtKbAEo7QAXCXW62rzsASOEvINae6eFA1R3lFCJ48HXJPor2Epx+MxKz74bkPF68M7Gqv0IiIipz0FlmD45mCJiIGIqICPfOsJ+WtYvCOEzk6KAV+TUJVhzb6p+d0ek/ySSrPd7v+84vXhbxqr9CIiIqc9BZZg+OdgSar2UcWKzb4moUo1LP55WAKbhCLCbMR6r/M3C5WXQPaWipMUWERERPxCDixr1qxh6tSpdOzYEcMwePvtt+u9ZvXq1QwfPhyn00n37t156aWXqp2zZMkS+vXrh8PhoF+/frz11luhFq3p1DJCCCpPHOfiWFEZx4utGpNuHSo3CR2z+sFUkhhTpR9LdkbFatAAuQosIiIiPiEHlqKiIgYPHsxzzz0X1PmZmZlccsklTJgwgc2bN3Pfffdx9913s2TJEv8569evZ8aMGcycOZMtW7Ywc+ZMpk+fzmeffRZq8ZpGUDUsbn/tSsd4p7WSs6+GxeOy+sFUUrFiszew+JqDfN9xeHsj/gAREZHTW1ioF0yZMoUpU6YEff5LL71E586dmT9/PgB9+/Zl48aNPPHEE1x5pTV8d/78+Vx00UXMnTsXgLlz57J69Wrmz5/PwoULQy1i4ys6Yu1rCCyVhzX7RwgleVdqjoiCMKfVYffkMXDG+a+rWLHZG1i+8waWwT+Cdc/AiSxrpFBEdBP8IBERkdNLk/dhWb9+PZMmTQo4NnnyZDZu3Eh5eXmd56xbt67W+5aWlpKfnx+wNZk6moQqD2vefcSqYTk7qVLIqKUfi2+k0NGiMqu5aL93hFDvKZVqWTRSSEREBJohsOTk5JCSkhJwLCUlBZfLxZEjR+o8Jycnp9b7zps3j/j4eP+Wnp7e+IX3qatJKKJyk1CVGhao6MdSS2A5XlQGefuhMAdsYdBxKCT1sU5Sx1sRERGgmUYJGYYR8N70dkCtfLymc6oeq2zu3Lnk5eX5t/379zdiiavwDWuuqdNtQJNQpRFCPpU73lYSMHmcr/9K6kAIj1RgERERqSLkPiyhSk1NrVZTkpubS1hYGO3bt6/znKq1LpU5HA4cDkfjF7gm/hqWmgKL9QiLy9z+RQ8DalhqaRIKWLHZF1jSR1v7ZG9g0UghERERoBlqWMaOHcuKFSsCji1fvpwRI0YQHh5e5znjxo1r6uIFp64aloiKzFfuNnGG20iLc1acUEsNS8CKzb4Ot51GWnt/DYtGComIiEADalgKCwvZtWuX/31mZiYZGRkkJibSuXNn5s6dy4EDB3j99dcBuPXWW3nuueeYM2cON998M+vXr+eVV14JGP1zzz33cO655/L4449z+eWX884777By5Uo++eSTRviJp6isGMqspp6a+rA4w23YDPDNsN+tQww2W6WmrHpqWAoKCqB8q3UwfZS1T+pr7TVSSEREBGhADcvGjRsZOnQoQ4cOBWDOnDkMHTqUBx98EIDs7GyysrL853fr1o1ly5axatUqhgwZwv/93//xzDPP+Ic0A4wbN45Fixbx2muvMWjQIBYsWMDixYsZPXr0qf6+U+cbIRTmBEdstY8NwwioZQkYIQT11rB0OvmNNU9LTCrEezsOR7fXSCEREZFKQq5hOf/88/2dZmuyYMGCasfOO+88vvjiizrve9VVV3HVVVeFWpymV+htDopOhlo6AUc7wijwriUU0H8F6h3W3M+9w4qN6SMD75/Ux2qKOvwNnDXslH+GiIjI6UxrCdXHPwdL9eYgH99IIaiphqXmBRBjHGGE2w2G2XZaBzqNCrxOI4VERET8FFjqU8cIIR/fSCGA7h2q1LD4m4SOBxw2DIPEqHCG2b61DqRXCSwaKSQiIuKnwFIf/wihOmpYKvVh6Va1hiUywdpXaRIC6Os8TpKRj8cWDmlDAj/USCERERE/BZb61DHLrY+vSSg1zumfqt/PV8NSXgSu0oCPRoZbo61OxPeFcGfgdVVHComIiJzBFFjqUxR8k1D3qrUrAI54MLyPuUotywC3NQLoYMzAGm6qkUIiIiI+Ciz18a3UXEeTUFREHYHFZqtoFqoytLlnmdXck+nsV/ON1fFWREQEUGCpX2waJHSF2I61njK5fwpd20cxdVAt59Q0tLmsiNSTVpPQ1/Y+NV+nwCIiIgI0w1pCp72rXqn3lPN7J7Pq57U3GRHVHo7uDBzafHAzNtxkm4nsLU+o+TqNFBIREQFUw9I8aprtdv9nAGzy9LTWE6qJr+OtRgqJiMgZToGlOdTUJLR/AwCbPT05WlRaw0VUNAlppJCIiJzhFFiaQ5Sv06138jjT9K/Q/IWnJ8eLy2u+TiOFREREAAWW5lG1huXYHig+immP4GuzK8eLy3B7almfSR1vRUREFFiaRdU+LN9ZzUFm2mDKCMc0Ie9kLbUsCiwiIiIKLM3CX8PiHSW032oOsqWPJj4yHIBjtfVj0UghERERBZZm4V+x2VvD4g0sdBpJYnQEAMeKaqth0UghERERBZbmULlJqLQAcr+23qePrhRYNFJIRESkNgoszcHXJHTyBHy3EUwPxKdDXBoJUVZgOVrbXCwaKSQiIqLA0ix8awlhwq6V1stOIwFo761hOV5bYAF1vBURkTOeAktzCIuAiFjr9bcfWvv0UQAkRNdTwwIKLCIicsZTYGkuvsnjjloLHtLJCixB1bBopJCIiJzhFFiai68fC0CYE1IHAvg73dZdwxI4UuiLrOOs2pHbJMUUERFpjRRYmotvaDNAx6FWMxEVgeV4cRBNQieyKC3O5/pXPufGBRs4cOJkU5VWRESkVVFgaS5RlWpYvB1uoSKwHCusI7BUGim0Z/sXFJS68Jiw9bsTTVFSERGRVkeBpblUbhLydriFwCYh06xlPSHw17Jk78zwH/r6YH6jFlFERKS1UmBpLgE1LNUDS6nLw8lyd+3XJ1v9WEoObvMfUmAREZEzhQJLc/HVsLTrDLEp/sNREXYcYdb/DEfrahZK6g1AdP5O/6FtCiwiInKGUGBpLp2Ggy0M+l0ecNgwjCA73lo1LN09+4kMtwOQk1/C0cJapvQXERFpQxRYmstZw+GXe+Gi/6v2UVBDm71NQum2w4zv7KRr+ygAtmWrlkVERNo+BZbm5IgFw6h2ODGYyeOiEsm3W5PPTUzKo3/HeED9WERE5MygwNIKVKzYXHtgMU2Tbz1nATAi6hD9OsYB6sciIiJnBgWWVqDeFZuB746f5KvyjgB08ez3B5avD+Y1fQFFRERamAJLKxDMekKfZx5jl2nVsIQf3UH/NCuw7DlSRHGZq+kLKSIi0oIUWFqBxJj6a1g27D3Gt55O1pvD20mOc9IhxoFpwjc5Bc1RTBERkRbToMDywgsv0K1bN5xOJ8OHD2ft2rW1nnvDDTdgGEa1rX///v5zFixYUOM5JSUlDSneaScxKogalr3H+Nb0BpYTWVBWRH9/s5D6sYiISNsWcmBZvHgxs2fP5v7772fz5s1MmDCBKVOmkJWVVeP5Tz/9NNnZ2f5t//79JCYm8sMf/jDgvLi4uIDzsrOzcTqdDftVp5n6Ot0eKSxlz+EiThCLJ8paU4jDO9TxVkREzhghB5Y//vGPzJo1i5tuuom+ffsyf/580tPTefHFF2s8Pz4+ntTUVP+2ceNGjh8/zk9+8pOA8wzDCDgvNTW1Yb/oNOQPLLVMHLdx7zEAeqfEYkv2rtx8+Bt/Dcs2dbwVEZE2LqTAUlZWxqZNm5g0aVLA8UmTJrFu3bqg7vHKK68wceJEunTpEnC8sLCQLl260KlTJy677DI2b95c531KS0vJz88P2E5XvsByorgcl9tT7fMNe48DMLJbgn8COQ5/Qz9vx9tvcgpqvE5ERKStCCmwHDlyBLfbTUpKSsDxlJQUcnJy6r0+OzubDz74gJtuuingeJ8+fViwYAHvvvsuCxcuxOl0Mn78eHbu3FnLnWDevHnEx8f7t/T09FB+SqvSLirCP5/c8eLyap9v8NawjOya6F9TiNxv6No+mqgIO6UuD3uOFDVXcUVERJpdgzrdGlVmazVNs9qxmixYsIB27doxbdq0gONjxozh2muvZfDgwUyYMIE333yTXr168eyzz9Z6r7lz55KXl+ff9u/f35Cf0irYbQbtIsOB6usJFZW6/J1qrcDiq2HZjs1m0DdN87GIiEjbF1Jg6dChA3a7vVptSm5ubrVal6pM0+TVV19l5syZRERE1F0om42RI0fWWcPicDiIi4sL2E5n/vWEqqzY/EXWcdwek7PaRdKxXWRFk9CJLCgtrNSP5fRtEhMREalPSIElIiKC4cOHs2LFioDjK1asYNy4cXVeu3r1anbt2sWsWbPq/R7TNMnIyCAtLS2U4p3WaluxeUOm1Rw0qluidSAqEdp1tl7vWaWhzSIickYIuUlozpw5/OUvf+HVV19l+/bt3HvvvWRlZXHrrbcCVlPNddddV+26V155hdGjRzNgwIBqnz3yyCN89NFH7Nmzh4yMDGbNmkVGRob/nmeC2lZs/rxy/xWfPlOt/fZ36ZdmLYK4LTsf0zSbvqAiIiItICzUC2bMmMHRo0f5zW9+Q3Z2NgMGDGDZsmX+UT/Z2dnV5mTJy8tjyZIlPP300zXe88SJE9xyyy3k5OQQHx/P0KFDWbNmDaNGjWrATzo91bRic5nLw+asEwCM6pZQcXK/78Onz8OOD+l5ydOE2QxOFJdzMK+Es9pFNmexRUREmkXIgQXg9ttv5/bbb6/xswULFlQ7Fh8fT3Fxca33e+qpp3jqqacaUpQ2o6bJ4746mEepy0NCVDhnJ8VUnNxpFMSkQOEhnPv/R4/kGL7JKeDrA3kKLCIi0iZpLaFWoqYVm339V0Z0TQwchWWzQZ/LrNfb36mY8TZb/VhERKRtUmBpJdrHVG8S8s2/Mqpy/xWfft+39t+8z4DUaEAdb0VEpO1SYGklEqMdQEUNi8djVprhtobA0uUciEyE4qOMtu8ANLRZRETaLgWWVqLqis07cwvJO1lOZLjdP3Q5gD0M+lwCQI+j/wXgwImTnKhlPSIREZHTmQJLK5EYU9Hp1jRN/3DmYV3aEW6v5X+mvpcD4Pj2fdLbWTU06sciIiJtkQJLK+GrYSlzeygqc/s73I6sqf+KT/fzwBEHhTlc1v4AoGYhERFpmxRYWonICDuR4XYAjhWWsbGmCeOqCnNAr4sBuMj8DFDHWxERaZsUWFoR31wsWw/kcTCvhDCbwdDO7eq+yDtaqO+JVYCpGhYREWmTFFhaEV9g+fBra3HJ/mfFExVRz9x+Z18I4VFEFh9ggJHJrsOFlJS7m7qoIiIizUqBpRXxBZaPv8kFYFTXhLpOt0REQc+LAPiBYxNuj8m3hwqarIwiIiItQYGlFfEFlsJSF1BP/5XK+lrNQpeEbQBM9WMREZE2R4GlFfEFFp8RwQaWXpPB7iDN9R09jQN8fTCvCUonIiLSchRYWpHKgaVHcky1AFMrRyyc/T0Aptg+V8dbERFpcxRYWpHKASXo5iAf72ihKfbP2Z5dgNtjNmbRREREWpQCSytSObCM6hZEh9vKel2MaQujry2LZNcB9h4tauTSiYiItBwFllbklGpYohIxuk4ArGYhdbwVEZG2RIGlFenaPpowm0GvlBg6JUSFfgNvs9DFdvVjERGRtkWBpRVJinWwYs55LLx5TMNu0OcyTAyG2PaQk7WzcQsnIiLSghRYWpluHaJpH+No2MUxyRSmjgSgU85KTFMdb0VEpG1QYGljHAN/AMC57vXkFpS2cGlEREQahwJLGxMx4HIARhjfsnP3rtpPLC2Er5bCigch/2AzlU5ERKRh6llZT0478Wex19mXriXbcX39HgwdUPFZaQF8+xF8/RbsWgmuEuv43k/gxo/AHt4yZRYREamHAksblHPWJLru3k7qgeVQcjvs+BC2vQ27/gPuSs1ECd2g+Cgc2ARr/gAX3NdiZRYREamLAksbZO8/DXY/Tc+TGfCHHuAuq/iwfQ/oNw36XQ6pA+GrJbBklhVYzr4QOo9uoVKLiIjUToGlDTq79wC+9HRjkC3TCisdekP/aVZISe4HhlFx8sCrYOdy+HIxLL0ZbvuftTaRiIhIK6LA0gYlRkdwq+On9CnewA+u+BFDh4+t+4JL/gD71sOJffDBr2Da881TUBERkSBplFAb1anHQF53T+ZvuyLrP9kZD1f8CTAg4w3Y9k6Tl09ERCQUCixt1A3juwLw7paD5OSV1H9Bl3Fwzr3W6/fu0VBnERFpVRRY2qhBndoxqmsiLo/JX9fvDe6i8+dC2hA4eRzevg08nqYsooiISNAUWNqwWRO6AfD3T/dRVOqq/4KwCLjizxAWCXtWwWcvNW0BRUREgqTA0oZN7JtC1/ZR5Je4WPLFd8FdlNQLJj9qvV75MBz6usnKJyIiEiwFljbMbjO48RyrluWVTzJxe4JcDHHEjdDrYmuSuSU3Q3kQfWBERESakAJLG3fV8E7ER4az72gxK7cfCu4iw4DvPwfRSZD7NfznN01bSBERkXo0KLC88MILdOvWDafTyfDhw1m7dm2t565atQrDMKpt33zzTcB5S5YsoV+/fjgcDvr168dbb73VkKJJFVERYVwzujMAr6zNDP7CmCQrtAB8+jzs/rgJSiciIhKckAPL4sWLmT17Nvfffz+bN29mwoQJTJkyhaysrDqv27FjB9nZ2f6tZ8+e/s/Wr1/PjBkzmDlzJlu2bGHmzJlMnz6dzz77LPRfJNVcP64r4XaDz/ceY8v+E8Ff2PtiGDHLer3yoSYpm4iISDAM0zSD7NhgGT16NMOGDePFF1/0H+vbty/Tpk1j3rx51c5ftWoVF1xwAcePH6ddu3Y13nPGjBnk5+fzwQcf+I9dfPHFJCQksHDhwqDKlZ+fT3x8PHl5ecTFxYXyk84IcxZnsHTzAaYO7sizPxoa/IVFR+HJ3uAph9vWQUr/piukiIiccYL9+x1SDUtZWRmbNm1i0qRJAccnTZrEunXr6rx26NChpKWlceGFF/Lxx4HNC+vXr692z8mTJ9d5z9LSUvLz8wM2qZ1viPOyrdkcOHEy+Auj20OvydbrjH80QclERETqF1JgOXLkCG63m5SUlIDjKSkp5OTk1HhNWloaL7/8MkuWLGHp0qX07t2bCy+8kDVr1vjPycnJCemeAPPmzSM+Pt6/paenh/JTzjj9O8Yz7uz2uD0mf123N7SLh1xj7b98E9zljV42ERGR+jSo061RebVfwDTNasd8evfuzc0338ywYcMYO3YsL7zwApdeeilPPPFEg+8JMHfuXPLy8vzb/v37G/JTzig3eWtZFn6WRWEwE8kBm/Yd4+qPYyiJSISiXNj1n4YX4KP74V83gju47xYREfEJKbB06NABu91ereYjNze3Wg1JXcaMGcPOnTv971NTU0O+p8PhIC4uLmCTup3fK5nuSdEUlLpYvKH+gLfw8yyufvlTPt1XwFvu8dbBjL837Mv3b4D1z8FXS+DAxobdQ0REzlghBZaIiAiGDx/OihUrAo6vWLGCcePGBX2fzZs3k5aW5n8/duzYavdcvnx5SPeU+tlsBjed0x2A1/6Xictd81pBZS4Pv357K3OXbqXcbfXJ/muxN7Ds+ACKj4X+5f+bX/E6a33o14uIyBktLNQL5syZw8yZMxkxYgRjx47l5ZdfJisri1tvvRWwmmoOHDjA66+/DsD8+fPp2rUr/fv3p6ysjDfeeIMlS5awZMkS/z3vuecezj33XB5//HEuv/xy3nnnHVauXMknn3zSSD9TfK4YdhZ/+Ogbvjt+kuXbDnHJwLSAz48UlnL737/g88xjGAb8bFJvdh8uZOkXcDCyFx1Pfgtb/wWjbwn+S4/shG/er3ifpeHqIiISmpD7sMyYMYP58+fzm9/8hiFDhrBmzRqWLVtGly5dAMjOzg6Yk6WsrIyf/exnDBo0iAkTJvDJJ5/w/vvvc8UVV/jPGTduHIsWLeK1115j0KBBLFiwgMWLFzN69OhG+IlSmTPczswx1v9Wf167J+Czrw7k8f1nP+HzzGPEOML4y3UjuOOCHkwd3BGAf5Q2sFlo3TOACQldrff7P4PQRtOLiMgZLuR5WForzcMSvMMFpYx/7L+UuT0suW0cw7sk8E7GAX7xry8pdXno3iGal68bQY/kGADK3R5GProSW/FRNkbeic10wW3rIaVf/V9WkAPzB4K7DK5/D/7+Q3CVwB0brIUWRUTkjNYk87BI25AU62DaUKvW5OU1u5m3bDv3LMqg1OXhgt5JvHXHeH9YAQi325gyII1jxLE9dqx1cEuQc7J8+qIVVtJHQ7dz4azh1vH9nzbmTxIRkTZOgeUMNcvb+fajrw/xpzVW09Dt55/NX64fSXxkeLXzpw62+rq8nO8LLIvrH55ckg8bX7Vej7/H2qd7m/nUj0VEREKgwHKG6p0ay7m9kgBwhtt49kdD+cXFfbDbap77ZnS39iTFOni/ZABlDu+cLLvrmZNl0wIozYcOvaDXFOtY5zHWXjUsIiISAgWWM9jvfjCAWed0Y+lt4/0da2tjtxlcOjANF2Gsj77QOlhX51tXKXz6gvV63N1g8/5T6zTS2h/dBUVHTvEXiIjImUKB5QzWKSGKBy7rR7+OwXVS9oWap4+MsA7UNSfL1n9CQTbEpsGg6RXHoxIhqY/1er+ahUREJDgKLBK0YZ3bcVa7SL4oSyc/vq/VmfarJdVP9Hjgf09br8fcBmGOwM/9/VjULCQiIsFRYJGgGYbBZYOszrcfhX/POlhTs9C3H8KRb8ERB8N/Uv1zfz8W1bCIiEhwFFgkJL5moSdzBmPawuDgZsjdHniSr3ZlxI3grKG5yVfDcnAzlJc0YWlFRKStUGCRkPTvGEe3DtHkuGLIST7XOphRaU6WrE+tEUD2CKs5qCaJ3SE6yWpSOri56QstIiKnPQUWCYlhGEz1Ngst8ZxnHfyy0pwsvtqVwVdDbGptN6moZdHwZhERCYICi4TM1yz0woHueCLbQ+Eh2P1fyP0GdiwDDGsoc118/Vg0gZyIiARBgUVC1jMllj6psRS77exK8U4Il/F3WPes9brPpdChZ9036eydMVcLIYqISBAUWKRBfLUsfz3pXcF5xzKraQhg/Oz6b5A6CMKccPIYHNnZNIUUEZE2Q4FFGsQ3vHlhVjyupP5WB1pPOXQZD+kj679BWIQWQhQRkaApsEiDdGkfzaBO8XhM2Jx4ScUHvkUOg6GFEEVEJEgKLNJgUwdZzUIvHR9hTcHfdQL0nBT8DbQQooiIBEmBRRrsUm+z0H+y3GTP2gTXvWMNWQ6WFkIUEZEgKbBIg3VsF8nIrgkAvL/1ENjsod1ACyGKiEiQFFjklPhGC7235WDDbqCFEEVEJAgKLHJKpgxIw2bAlu/yyDpaHPoNtBCiiIgEQYFFTklSrIOxZ7cH4L0vG1DLooUQRUQkCAoscsp8o4Ve+18mCz/PotTlDv7ihiyEWJIHm99QR10RkTOIAoucsikD0+icGMWRwjLmLt3Keb9fxSufZFJc5qr/4lAXQvS4YeGP4Z074KVzYN/6Uyu8iIicFhRY5JTFR4bz4ewJ/PrSvqTEOcjJL+H//r2N8Y/9l2f/s5O8k+V138DbjyXv20/42/q9/HnNHsrdnprPXfMH2PeJ9bogGxZcaq0QrfWIRETaNMM028b/p8/Pzyc+Pp68vDzi4uJaujhnrFKXm6VfHODFVbvJOmZ1wo1xhDFzbBduHN+NpFgHLreHnbmFfPndCbZ8l0dZ5mc8kf9TjpkxDCv9E2DwyPf7c/24roE33/sJ/HUqmB647CmrdmXrm9ZnvS+BaS9AZEKz/l4RETk1wf79VmCRJuFye3h/azYvfLybHYcKAHCE2eiTFseOnHxKyitqUMJxsdUxC6dRzs1xL7IiN56zk6JZOec8DN9EdEVH4aXxVq3KkGuscGKasOk1+OCXVh+Ydp1h+uvQcWhL/GQREWmAYP9+q0lImkSY3cblQ87ig3sm8JfrRjAkvR2lLg9b9p+gpNxDjCOMsd3b8//O7c78H4+Cs4YB8Oz4UqIj7Ow+XMS63Uetm5kmvH2bFVba94Qpv7eOGwaMuBFmrYB2XeBEFrwyCT7/s5qIRETamLCWLoC0bTabwcR+KVzYN5mN+45z8MRJ+neMp3uHaGy2StP4546Dg5/hzN7IFcNu42+f7uP19XsZ36MDfPoC7PwI7A744QJwxAR+Scch8P/WWB1xv/k3LPsZZK2HqU+DI7Y5f66IiDQR1bBIszAMg5FdE7l8yFn0SI4JDCsA6RULIc4c2wWAFdsOcXjHeljxkPXZxb+D1AE1f0FkO5jxBkx6FGxh8NUSePkCOPR10/wgERFpVgos0jqkj7L2R3fRK6aUMd0TiTKLCV86Czzl0Pf7MGJW3fcwDBh3J9zwPsR2hKM7rdCy/nnw1DLqSERETgsKLNI6RCVCh97W6/2fcd2YLvwu/BXalR7AjE+H7z8b/ErQncfArWuh52Rwl8JH98HfpkHegSYrvoiINC0FFmk9fOsKZX3KpLLlfN++HpdpY+3g31tNPqGI7gA/XmwNfw6PgszV8OJYq6koVCf2w7E9oV8nIiKNpkGB5YUXXqBbt244nU6GDx/O2rVraz136dKlXHTRRSQlJREXF8fYsWP56KOPAs5ZsGABhmFU20pKtLbMGcUXWLa/S9iHvwLgCdd0nt7RrmH3840i+n9roeMwa0r/f90IS26GkyfqvrbwsDXa6JVJMH8APDtCo49ERFpQyIFl8eLFzJ49m/vvv5/NmzczYcIEpkyZQlZWVo3nr1mzhosuuohly5axadMmLrjgAqZOncrmzYHrxsTFxZGdnR2wOZ3Ohv0qOT35pug/vhdcJyntegGvMpVN+47z9cG8kG617WA+33tyFc/9dyd06AGzlsN5vwTDZk029+J4yKwStEsLYMsieONKeLK3NdrIt4q06bbe//tecNczc6+IiDS6kCeOGz16NMOGDePFF1/0H+vbty/Tpk1j3rx5Qd2jf//+zJgxgwcffBCwalhmz57NiRMnQilKAE0c1waYJjzRE4oOQ0wK3Po/7nrvO97bcpCrR6bz2JWDgrpNYamLy55Zy96j1ky7r/1kJBf0TrY+3P85LL0FjmcCBoy7y6rZ2fpP2PEhuE5W3KjjUBj4Q+h/hRVyVjwEmNDlHGuCuuj2jfv7RUTOQE0ycVxZWRmbNm1i0qRJAccnTZrEunXrgrqHx+OhoKCAxMTEgOOFhYV06dKFTp06cdlll1WrgZEzgGHAoBkQEQNX/BlikrjOO8T57YwD5BXXX7Nhmib3Ld3K3qPF2L1Dp3/25hZy873Ni+mj4NZPYNh1gAnrnoFFP4av37LCSvsecP5cuHMT3LIKxt4BcWkw/h740SKIiLXWMvrzBZC7PbTfl/OVNWKp4FBo14mISGiB5ciRI7jdblJSUgKOp6SkkJOTE9Q9nnzySYqKipg+fbr/WJ8+fViwYAHvvvsuCxcuxOl0Mn78eHbu3FnrfUpLS8nPzw/YpA2Y/Cj8ci90Pw+AEV0S6JMaS0m5h39u2l/v5Ys37OfdLQex2wz+NmsUfdPiOFpUxpw3t+DxeCsTHTHWqKOr/wHRyRCTCmPvtALKnRvh/F9ZzUhV9b4YbloBCV3hxD74y0VWrUxdXGWw9V/w6sXW0gIf3Qd//h4c2hbSYxEROdM1qNOtUWV4qWma1Y7VZOHChTz88MMsXryY5ORk//ExY8Zw7bXXMnjwYCZMmMCbb75Jr169ePbZZ2u917x584iPj/dv6enpDfkp0hrZw/0vDcPgurFdAfjbp/sqQkcNduQU8NC71kRxP5/cm3Fnd+DZHw0lMtzOJ7uO8NKa3YEX9LkUfvqNtU1+1GoCqu/fcXJfuPlj6DoBygpg4dXwyfzqnXHzD8J/H7U67C6ZZc28a9itgJT/Hbw6GfasCvKBiIhISIGlQ4cO2O32arUpubm51Wpdqlq8eDGzZs3izTffZOLEiXUXymZj5MiRddawzJ07l7y8PP+2f3/9//Utp6dpQzsS6wxj39Fi1uw8XOM5xWUu7vjHF5S6PJzXK4lbJnQHoEdyDI9c3h+AJ5d/y6Z9xwMvtNmDn9/FJyoRZr5ljUDChJUPwVu3QvlJyFwDi2fCUwNgze+h8JBVg3Per+Der+GOz6DzOCjNtzr3Zvwj1McRvO3/hvUvQLlG24nI6S+kwBIREcHw4cNZsWJFwPEVK1Ywbty4Wq9buHAhN9xwA//4xz+49NJL6/0e0zTJyMggLS2t1nMcDgdxcXEBm7RNURFhXDW8EwB/W7+vxnMefOdrduUWkhLn4I/TBwdM/f/D4Z34/uCOuD0mdy/cTN7JRhjlYw+35ni55Amr5uTLRfCHnvDXqbD9XWtUUZfxcNVrcO9XcMFcqy+ML+wMuBI8LmtRx1WPNe5wadOEtU/C4mvgo7nwpwmwf0Pj3V9EpAWE3CQ0Z84c/vKXv/Dqq6+yfft27r33XrKysrj11lsBq+bjuuuu85+/cOFCrrvuOp588knGjBlDTk4OOTk55OVVDFN95JFH+Oijj9izZw8ZGRnMmjWLjIwM/z1FZo6xOt/+d0cu+48VB3y2ZNN3/GvTd9gMePrqobSPcQR8bhgGj/5gAJ0Tozhw4iT3Ld1KiIPjajfqZpi5FJztrCai8Gir5uW2dfCTZTDgioAmLgDCnXDFX+Cce633q+ZZCze6yk69PKYJy38N//mN9d4RB0e+hVcnWcfLT9Z9vYhIKxVyYJkxYwbz58/nN7/5DUOGDGHNmjUsW7aMLl2sPyjZ2dkBc7L86U9/wuVycccdd5CWlubf7rnnHv85J06c4JZbbqFv375MmjSJAwcOsGbNGkaNGtUIP1Hagu5JMUzo2QHThDc+q6hl2ZVbyAPvfAXA7Im9GNO95qHGsc5wnvnRUMJsBu9vzWbRhkZsQux+vjXy6MpX4KfbrZqXlP51X2OzwcSHrXMNG2T8Hf7xQ2tyu4Zyu+DdO2H9c9b7SY/CPVtg8I/A9MC6Z+GlcyDr04Z/h4hICwl5HpbWSvOwtH0rth3i5tc3khAVzvq5FwIw7fn/8U1OAePObs/fZo32D2WuzZ9W72beB9/gDLfx3p3n0DMltjmKXrdvl8M/b4DyIkjuB9f8E+I7hXaP8hKrc+83/7YC0PefhaHXVvqOj+C9e6AgGzBgzG3wvQcgIqoxf4mISMiaZB4WkZb0vT7JnNUukuPF5by35SC/+fc2vskpoENMBPOvHlJvWAG4eUJ3zu2VREm5hzv/sZmScvcpl+tIYSnLtmaz9IvvGna/XpOs5qOYVMjdBn++EHauBE+Q9yotsGpnvvk32COsSe0qhxWAXpPh9k9hyLWACZ++AC+Og73/C728IiItQDUsclp5YdUufv/hDtpHR3C0qAzDgL/dOJpzenYI+h6HC0qZ8vRajhSWcu2Yzvx22sCQypBbUMJne47xWeZRPt1zjF25hf7PBp4Vz4vXDqNTQgNqLk7sh7//EA57J6SLOwsGXw2Df1zzvDAAxces0UYHv7Am3Lv6H/45bGq1cyW8dzfke1evHnmz1Z8m/qzQyywicoqC/futwCKnlaOFpYyd91/K3B4A7rygBz+b3Dvk+6z59jDXvfo5YM3Z0rV9NHYb2G02wmwGdptRsbcbfHf8JJ96Q8qew0XV7tcnNZZD+SUcLy4nMTqC5340lHE9gg9RfiV58N/fwpeLA/uzdBoFQ34M/X9QsXJ13gH42w/gyA6ITIRr/wVnDQ/+e5Y/AF/81Xpv2KDnZBjxE+gx0RruLSLSDBRYpM2a82YGS784wMiuCSy8eQxh9oa1bM77YDt/Wr0n5OsMA/qmxjG6eyJjurdnVNdEEqIj+O54Mbe98QVbD+RhM+BXU/pw84TuQU2qWE15CexYBlsWwq6VVqdZgDCnNeFd70tg5cOQtx9iO8J1b0NS6MGN3R/Dmies5QZ84jpZSxcMmwlxHUO/p09ZEZzIshazPL4Xju+zZgh2xkPnsdaw7/Znhz4Pjoi0KQos0madKC7jX5u+48phnUiIjmjwfcrdHv644lu27D+By2PirrRZ7z24PCYej0l8ZDgjuyYy2htQ4qPCa7xnSbmbX7/9Ff/a9B0Alw1K4/dXDSIqIqzB5aQgB75805pk7nCV9YsSz7bCSrvODb8/wOFvrdqWjL/DSe/keoYNel0Mw38CPS60al1ME8qLofiotRUdrfT6sBWgju+zAkpRbv3fG5MCXcZZ4aXLOEjqa42gEpEzhgKLSAsxTZM3Pt3HI+9tw+Ux6Z0Sy59mDqdrh+h6r3V7TL47XkxKnBNneJVmGdOEg5utWpet/4LE7taCjDFJjVf48hLY/h5seg32VeqQG51sdegtPgKuEGbOdcZbay8ldIV2XaxgVXgI9q2D7zaCu7TK+e2s4JI+ClIGWKOm4jqqFkakDVNgEWlhG/Ye4/a/f8HhglLinGE8ffVQLuiTHHDO4YJSMvafYHPWcTL2n+DL7/IoLHURGW7ngj5JXDwgjQt6JxHrrKFGxzSb9g/54W9h0wLY8o+KWhcfewREdYCo9hDd3tpHJlrDsX0BJaELRCbUfv/yEquz8L7/WQEm6zNraHdVznbWvDbJ/ax9Sn9rTSdHlSHpHg+4y6xA5S4DV6k143B8uvrkiLRiCiwircCh/BJue2MTX2SdwDDgjvN7kBAd4Q8o3x2vPvOs3WbgrrTIY4Tdxjk9O3DxgFQu6ptySs1gDVJeAgc2QXikFUyi2kNEdOOHJXc55HxpDbU+uNka4n1kpxU6ahKdbH3m8oYUTy1LLjjioNNIq99M59Fw1ojg5p8xTSg6Ys0UfHSnFcjOvqB6UBKRU6LAItJKlLk8PPLe1/z9s6xqnxkG9EyOYWh6AkM6t2No53b0SIphW3Y+H36Vw4df5bDnSEWtg91mMLpbohVe+qWQFh/ZnD+l+ZWXWIEhdxsc+goObbNeF2TXf63dAZhWbUtltjBIGwzpY6DzGKv5qazI+h7fdti7LzlR5Z4R0O1c6D0Fek3RUHCRRqDAItLK/HPjfv726T6SYx0M7ZzAkPR2DOoUX3Nzj5dpmuzMLfSHl23Z+QGf90iO4ZweHTi3VwdGd2tPtOMUOveeToqPWR187REQ5rDCSZij4rU93EqDbhfkfm0tR+DbCg6G8EWG1e+mfQ84ngnHqowqSxtsjdjqfQmkDjz1WifTtIacFx2xhq9HJqoTsrR5CiwibdC+o0V89LUVXjL2n6BSyxHhdoNhnROY0LMDE3omMeCseP/svx6PyfHiMg7ll3KooITc/BJyva/LXSYTenXg/N7JxLT1wGOaVtCpHGByt1lBp31P6NDTGh7eoSd06GUFlfDIimuP7LSGm+/4APZ/BlT6HyCuE3Qdb/W5ccRYTUeOWIiIrXjtiLFqeApyIO87yD/o3Sq9LquYiBBbmNX0FZNsjajy772vw5zWeYYBGJUCkwGGd4/pLabpXRW8hr3HbTWpuV3efbm1mri7vOK9zW6Ft8TukNCtYj6gxv7fpyTP6jPljLdWN5c2T4FFpI3LKy5n3e4jrN11hLU7D7P/WGB/mPjIcLp2iOZIQSm5BSWUu+v+P/WIMBvn9uzA5P6pTGyJvjItpfykVSsTak1G4WHY+ZEVXnb/1xru3VgiYgKDS2sUmVARXhK7Q2I37/B6o1LH55KKPkb+9yVWKCk+DiePWbVl/v3xwD5LHXpB+mhr6zzGCpCN0XfKVQpHd1vTBBzeYc0XlNQbuk6was2qrrAuTUqBReQMs+9oEWt3WuFl3e6jFJS4qp3TISaC5FgnKXEOUuKcJMc6KHF5WP51DnuPVvzBtdsMxnRP5OL+qUzqn0pKnLPavUzTpLjMTWGpi8JSF8WlbrolRbf9WpqalJ+EzDVWbU1pobW+U2kBlBVUvPYdd5dBbJo1XDv+LGsf18m7976PiLJqNYoOW8PAC3OtfcEh73vvMXcZ1WtMIKBWpXJtS0BNTKW9zQ62cLCHeffhVu2OPbziuLvcO8dOpvX9TSksElzVO6QTmegNMKOsfXJf67jHbU2uGLB5j5UWevslfQO53oBybE/tnbnDo61w1PUca+s4tOYA43FbtXVHdnq3b+HoLut4XFrF/8aV97FpENZI/yFgmta/uzawgKkCi8gZzOX2sOW7PI4UlvqDSVKsg/BaZgU2TZNvD3n7ynydw/YqfWUGnBVHhN1mhZMSFwWlLopKXQFNUmCNaJrQswOTB1i1NIlnSi3Nmaa00Joc8Ngea/P178n7Dgy71VQVFuHdV+1jFFHR3BOZWGWfYL0Od1qTEn73udX0lvWZNQQ+lDmA6uOIg6Q+Vs1KfLp3hNon1Ttah0dbAanLeCsgHvnWCijHdjesPFEdrCY9Zzw446y9I676+zCnd1LGI1afpqLDlTbvRI2ecojvbAWszmOsOYw69D7t+j0psIhIg1XuK/NF1ok6z7UZEOMII9xu42hRxYicyiOaJvVLJTW+ei2NSNBcZVao2P+Z1fdo/2dVanoMa3bmypvNXtE/Kam3FVCS+1j72LTqzUsej9VJe+//YO9aa36gk8dqL5M9wmqmat/Dar7q0NOqjcnPtkay5R8M3FcdsdYUnO0qAkznsVYNUZjDqpEpK/LW/BVCaX5FrV+Zb19kvfbtSyu99h2f8UZFzVYjUWARkUZxKL+EDXuPEWazEesMI8YRRowzjFjvPjLcjmEY9Y5oGtq5HRf3T+XspBiKy92UlLkpLnNRXO7mZJmbYu92ssyFzTBoHxNBhxgH7WMctI+JIMm7bx/tICLs9PovSGkCpml1DPYHlCaYRNHjsfq57P2fFZAiorzBxBtO2nUJflJC07T66RQctJrzSvOhJN+7z7Nel+RVHHedtGqbopMgukPN+/AoyN7i7UDunT26al8qu7emq7SAgE7iDfWTD6HL2FO/TyUKLCLSorKOFlu1NF/nsGnf8fovCEGcM4wOMQ7aRYWTGB1Bu6gI7z6cxKiK9/GRVt8Dt8fEY5r+vfXaOm6aJs4IO3HOMGKd4cQ5w3GG2xq2aKVIS/JNvpj1KWSth33rrSalygxbpZFr3tFsETHW64hYa1LIiGjve98WXXFOSn+r2aoRKbCISKuRm1/C8m2HWLHtECeKy4iMsBMVYdXOWK+9+/AwoiLsuDwmx4pKOVpYxuFCa3/U+95VteNMEwizGcRFhhPrDCPWGUacM9yqWXKEEe3dYhz2Sq8r9vGR4f5NNUHSokzT6mvkcVcElPCoVrc2lwKLiLQ5Ho9Jfkk5RwrLOFpYyvHico4Xl3GsqIwTxWUcKyq39sVlnCi2XhuGgc0wsNvAbhjYbAZ2m+F/bQAny90UlLgoKCmv1pH4VESG2/3hJS7SCjOxznDC7QZhdhvhNmsfZjcIt3n3dhsRdhvOCDtR4XaiHXYiI6wgF+UNetHegOcMtxNmM1QbJKe1YP9+n4HjD0XkdGWzGbTzNvn0SI5p9PubpklRmZuCknIKSlzkn/TuS8op9I6MKixxUVjqtl6XWceKSq1jBSXl5HmvASsInSx3k5PfiKNbqrAZ4Aiz4wi3go4j3Ga9D7PhCLMR7g1EYTabFZRsvvfe0GQ3iLDbiPQGoShHzeEoKiIMZ7gNZ7gdp/f7HGFqOpPmo8AiIuJlGIa/6SftFJrp3R6TgpJy8k+6yDtZHrAVlpZT7jZxuU1cHg/lbpNytweX20O5x7T2bpOTZW6KylyVOiS7/B2Ti8sqhpR7zIpg1NwMAxxhgSHG9/tNk4A+Q55K753hdn+n7eiIik7c0d5jMY4wHGE2bIaBzbCCquF77d0bhlVLFma3aszCbAZ2m827Nyr2doMIu52IMBsR3hDnex1hV+g6nSiwiIg0MnulmqCmYJompS4PpeUeSl1u67Wr0utKx60wZIUit8f0hyKX26TcY+1LXb4RWm6KvCO1iiu9Liq1AlGJd/OFJdOEknIPJeUeoJbVsmtQUOLicEFpkzybhgj3hh6bYQSEpMrv7TYDZ7hV8xQdEUaUw7uP8PVlsmqhIrxzHfkykC8MGd5j/sUTDCPgPYbhPwes5ktHuA1nmNX05/DWbvkDYrgdZ1jFsbBa5lhqSxRYREROM4Zh+P9oQfNOI2+aJuVukxKXFV5Kyz2UeGt4Sl2egNoPm2Fgs+HtQ1RRM1JS7qawxEVRmYuCEle15raCknLK3R48JnjMqrU1Vhk8ponLe8zltkaAuTyV9x5cHuuzMpeHMrfH2ntfV2bVcp3e3TntNgNnmA1HeEVzoC/MmOCv0fM9E7enUoh1ewiz24gMt+MMtwV0iI/0hjSn9/X1Y7vSuX3LzK6rwCIiIkEzDIOIMIOIMBtxdaw03pp5PKYVYCqFGI9p4vFYAcltWsPd3b733mBUUu6hqMxahsLaW/MI+d4XlbpwuU1MrFDlXyjBu1KCb4yLfy1K7xmm6Tun4r3HW4tm1Wp5966K16XlgcHL7bH6XxWVNW3T4GWD0hRYREREmoPNZuC0+WqoTl8ejxnQFFhSXtEkWOKqqP2y2fD37wnz9usJs9mw26xRaXabgce7NtjJMqvmrLjMqjU7Weby7j0Ul7tIi49ssd+rwCIiInIastkMq9km4vQOXsFq+710RERE5LSnwCIiIiKtngKLiIiItHoKLCIiItLqKbCIiIhIq6fAIiIiIq2eAouIiIi0eg0KLC+88ALdunXD6XQyfPhw1q5dW+f5q1evZvjw4TidTrp3785LL71U7ZwlS5bQr18/HA4H/fr146233mpI0URERKQNCjmwLF68mNmzZ3P//fezefNmJkyYwJQpU8jKyqrx/MzMTC655BImTJjA5s2bue+++7j77rtZsmSJ/5z169czY8YMZs6cyZYtW5g5cybTp0/ns88+a/gvExERkTbDMH2LGwRp9OjRDBs2jBdffNF/rG/fvkybNo158+ZVO/+Xv/wl7777Ltu3b/cfu/XWW9myZQvr168HYMaMGeTn5/PBBx/4z7n44otJSEhg4cKFQZUrPz+f+Ph48vLyiIuLC+UniYiISAsJ9u93SDUsZWVlbNq0iUmTJgUcnzRpEuvWravxmvXr11c7f/LkyWzcuJHy8vI6z6ntngClpaXk5+cHbCIiItI2hRRYjhw5gtvtJiUlJeB4SkoKOTk5NV6Tk5NT4/kul4sjR47UeU5t9wSYN28e8fHx/i09PT2UnyIiIiKnkQZ1ujUMI+C9aZrVjtV3ftXjod5z7ty55OXl+bf9+/cHXX4RERE5vYS0WnOHDh2w2+3Vaj5yc3Or1ZD4pKam1nh+WFgY7du3r/Oc2u4J4HA4cDgc/ve+EKSmIRERkdOH7+92fV1qQwosERERDB8+nBUrVvCDH/zAf3zFihVcfvnlNV4zduxY3nvvvYBjy5cvZ8SIEYSHh/vPWbFiBffee2/AOePGjQu6bAUFBQBqGhIRETkNFRQUEB8fX+vnIQUWgDlz5jBz5kxGjBjB2LFjefnll8nKyuLWW28FrKaaAwcO8PrrrwPWiKDnnnuOOXPmcPPNN7N+/XpeeeWVgNE/99xzD+eeey6PP/44l19+Oe+88w4rV67kk08+CbpcHTt2ZP/+/cTGxtbZlBSq/Px80tPT2b9/v0YfNQM97+al59289Lybl55382ro8zZNk4KCAjp27FjneSEHlhkzZnD06FF+85vfkJ2dzYABA1i2bBldunQBIDs7O2BOlm7durFs2TLuvfdenn/+eTp27MgzzzzDlVde6T9n3LhxLFq0iF//+tc88MADnH322SxevJjRo0cHXS6bzUanTp1C/TlBi4uL0z/4ZqTn3bz0vJuXnnfz0vNuXg153nXVrPiEPA/LmUbzuzQvPe/mpefdvPS8m5eed/Nq6uettYRERESk1VNgqYfD4eChhx4KGJEkTUfPu3npeTcvPe/mpefdvJr6eatJSERERFo91bCIiIhIq6fAIiIiIq2eAouIiIi0egosIiIi0uopsNTjhRdeoFu3bjidToYPH87atWtbukhtwpo1a5g6dSodO3bEMAzefvvtgM9N0+Thhx+mY8eOREZGcv755/P111+3TGFPc/PmzWPkyJHExsaSnJzMtGnT2LFjR8A5et6N68UXX2TQoEH+CbTGjh3LBx984P9cz7vpzJs3D8MwmD17tv+YnnfjevjhhzEMI2BLTU31f95Uz1uBpQ6LFy9m9uzZ3H///WzevJkJEyYwZcqUgJl8pWGKiooYPHgwzz33XI2f//73v+ePf/wjzz33HBs2bCA1NZWLLrrIv2aUBG/16tXccccdfPrpp6xYsQKXy8WkSZMoKiryn6Pn3bg6derEY489xsaNG9m4cSPf+973uPzyy/3/T1vPu2ls2LCBl19+mUGDBgUc1/NufP379yc7O9u/bd261f9Zkz1vU2o1atQo89Zbbw041qdPH/NXv/pVC5WobQLMt956y//e4/GYqamp5mOPPeY/VlJSYsbHx5svvfRSC5SwbcnNzTUBc/Xq1aZp6nk3l4SEBPMvf/mLnncTKSgoMHv27GmuWLHCPO+888x77rnHNE39+24KDz30kDl48OAaP2vK560allqUlZWxadMmJk2aFHB80qRJrFu3roVKdWbIzMwkJycn4Nk7HA7OO+88PftGkJeXB0BiYiKg593U3G43ixYtoqioiLFjx+p5N5E77riDSy+9lIkTJwYc1/NuGjt37qRjx45069aNq6++mj179gBN+7xDXvzwTHHkyBHcbjcpKSkBx1NSUsjJyWmhUp0ZfM+3pme/b9++lihSm2GaJnPmzOGcc85hwIABgJ53U9m6dStjx46lpKSEmJgY3nrrLfr16+f/f9p63o1n0aJFfPHFF2zYsKHaZ/r33fhGjx7N66+/Tq9evTh06BC//e1vGTduHF9//XWTPm8FlnoYhhHw3jTNasekaejZN74777yTL7/8kk8++aTaZ3rejat3795kZGRw4sQJlixZwvXXX8/q1av9n+t5N479+/dzzz33sHz5cpxOZ63n6Xk3nilTpvhfDxw4kLFjx3L22Wfz17/+lTFjxgBN87zVJFSLDh06YLfbq9Wm5ObmVkuO0rh8vc317BvXXXfdxbvvvsvHH39Mp06d/Mf1vJtGREQEPXr0YMSIEcybN4/Bgwfz9NNP63k3sk2bNpGbm8vw4cMJCwsjLCyM1atX88wzzxAWFuZ/pnreTSc6OpqBAweyc+fOJv33rcBSi4iICIYPH86KFSsCjq9YsYJx48a1UKnODN26dSM1NTXg2ZeVlbF69Wo9+wYwTZM777yTpUuX8t///pdu3boFfK7n3TxM06S0tFTPu5FdeOGFbN26lYyMDP82YsQIrrnmGjIyMujevbuedxMrLS1l+/btpKWlNe2/71PqstvGLVq0yAwPDzdfeeUVc9u2bebs2bPN6Ohoc+/evS1dtNNeQUGBuXnzZnPz5s0mYP7xj380N2/ebO7bt880TdN87LHHzPj4eHPp0qXm1q1bzR/96EdmWlqamZ+f38IlP/3cdtttZnx8vLlq1SozOzvbvxUXF/vP0fNuXHPnzjXXrFljZmZmml9++aV53333mTabzVy+fLlpmnreTa3yKCHT1PNubD/96U/NVatWmXv27DE//fRT87LLLjNjY2P9fxub6nkrsNTj+eefN7t06WJGRESYw4YN8w8FlVPz8ccfm0C17frrrzdN0xoa99BDD5mpqammw+Ewzz33XHPr1q0tW+jTVE3PGTBfe+01/zl63o3rxhtv9P//jaSkJPPCCy/0hxXT1PNualUDi55345oxY4aZlpZmhoeHmx07djSvuOIK8+uvv/Z/3lTP2zBN0zy1OhoRERGRpqU+LCIiItLqKbCIiIhIq6fAIiIiIq2eAouIiIi0egosIiIi0uopsIiIiEirp8AiIiIirZ4Ci4iIiLR6CiwiIiLS6imwiIiISKunwCIiIiKtngKLiIiItHr/H9pwe8VxsI/4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0879 - mse: 0.0879 \n",
      "Test Loss: 0.08408260345458984\n",
      "Test MSE: 0.08408260345458984\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funzione per costruire il modello (modificato per Keras Tuner)\n",
    "def build_rbf_model_tuner(hp):\n",
    "    input_dim = 12  # Numero di caratteristiche di input\n",
    "    output_dim = 3  # Numero di target (x, y, z)\n",
    "\n",
    "    n_centers = hp.Choice('n_centers', [25])\n",
    "    gamma = hp.Choice('gamma', [0.1])\n",
    "    learning_rate = hp.Choice('learning_rate', [0.5])\n",
    "    batch_size = hp.Choice('batch_size', [75])\n",
    "\n",
    "    # Crea il modello\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Layer RBF\n",
    "    centers = tf.Variable(np.random.randn(n_centers, input_dim), dtype=tf.float32)\n",
    "    model.add(layers.Lambda(lambda x: rbf_activation(x, centers, gamma)))  # Applica l'attivazione RBF\n",
    "\n",
    "    # Layer di output\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    # Ottimizzatore\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Callback per visualizzare i progressi di ogni tentativo\n",
    "class PrintTrainingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}: Loss={logs['loss']}, Val_Loss={logs['val_loss']}\")\n",
    "\n",
    "# Tuner senza salvataggio dei risultati\n",
    "tuner = kt.Hyperband(\n",
    "    build_rbf_model_tuner,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    overwrite=True  # Evita di salvare i modelli e i risultati\n",
    ")\n",
    "\n",
    "# Esegui la ricerca con log dei progressi\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[PrintTrainingCallback()]  # Stampa i progressi\n",
    ")\n",
    "\n",
    "# Ottieni i migliori parametri\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(f\"Migliori parametri: {best_hp.values}\")\n",
    "\n",
    "# Costruisci e addestra il modello con i migliori parametri\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=best_hp['batch_size'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[PrintTrainingCallback()]\n",
    ")\n",
    "\n",
    "# Plot dei risultati\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mse'], label='Training MSE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    " # Valutare il modello sui dati di test\n",
    "test_loss, test_mse = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e5aee-4b57-492a-ae2f-b74d4aa2fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Funzione per costruire il modello (modificato per Keras Tuner)\n",
    "def build_rbf_model_tuner(hp):\n",
    "    input_dim = 12  # Numero di caratteristiche di input\n",
    "    output_dim = 3  # Numero di target (x, y, z)\n",
    "\n",
    "    n_centers = hp.Int('n_centers', min_value=20, max_value=30, step=5)\n",
    "    gamma = hp.Float('gamma', min_value=0.1, max_value=1.0, step=0.5)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=0.00005, max_value=0.05, step=0.001)\n",
    "    batch_size = hp.Int('batch_size', min_value=1, max_value=30, step=1)\n",
    "\n",
    "    # Crea il modello\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Layer RBF\n",
    "    centers = tf.Variable(np.random.randn(n_centers, input_dim), dtype=tf.float32)\n",
    "    model.add(layers.Lambda(lambda x: rbf_activation(x, centers, gamma)))  # Applica l'attivazione RBF\n",
    "\n",
    "    # Layer di output\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    # Ottimizzatore\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Callback per visualizzare i progressi di ogni tentativo\n",
    "class PrintTrainingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}: Loss={logs['loss']}, Val_Loss={logs['val_loss']}\")\n",
    "\n",
    "# Tuner senza salvataggio dei risultati\n",
    "tuner = kt.Hyperband(\n",
    "    build_rbf_model_tuner,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    overwrite=True  # Evita di salvare i modelli e i risultati\n",
    ")\n",
    "\n",
    "# Esegui la ricerca con log dei progressi\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[PrintTrainingCallback()]  # Stampa i progressi\n",
    ")\n",
    "\n",
    "# Ottieni i migliori parametri\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "print(f\"Migliori parametri: {best_hp.values}\")\n",
    "\n",
    "# Costruisci e addestra il modello con i migliori parametri\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=best_hp['batch_size'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[PrintTrainingCallback()]\n",
    ")\n",
    "\n",
    "# Plot dei risultati\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mse'], label='Training MSE')\n",
    "plt.plot(history.history['val_mse'], label='Validation MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    " # Valutare il modello sui dati di test\n",
    "test_loss, test_mse = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
